<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Lecture 2, Imitation Learning | Son-Dongwoo's Blog</title>
<meta name=keywords content="Deep Reinforcement Learning"><meta name=description content="UC Berkeley의 CS 285 강의인 Deep Reinforcement Learning의 Lecture 2 정리"><meta name=author content="Son-Dongwoo"><link rel=canonical href=https://son-dongwoo.github.io/posts/lecture-2/><meta name=google-site-verification content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://son-dongwoo.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://son-dongwoo.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://son-dongwoo.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://son-dongwoo.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://son-dongwoo.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://son-dongwoo.github.io/posts/lecture-2/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:url" content="https://son-dongwoo.github.io/posts/lecture-2/"><meta property="og:site_name" content="Son-Dongwoo's Blog"><meta property="og:title" content="Lecture 2, Imitation Learning"><meta property="og:description" content="UC Berkeley의 CS 285 강의인 Deep Reinforcement Learning의 Lecture 2 정리"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-02-05T01:10:35+09:00"><meta property="article:modified_time" content="2025-02-05T01:10:35+09:00"><meta property="article:tag" content="Deep Reinforcement Learning"><meta property="og:image" content="https://son-dongwoo.github.io/posts/lecture-2/images/cover.webp"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://son-dongwoo.github.io/posts/lecture-2/images/cover.webp"><meta name=twitter:title content="Lecture 2, Imitation Learning"><meta name=twitter:description content="UC Berkeley의 CS 285 강의인 Deep Reinforcement Learning의 Lecture 2 정리"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://son-dongwoo.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Lecture 2, Imitation Learning","item":"https://son-dongwoo.github.io/posts/lecture-2/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Lecture 2, Imitation Learning","name":"Lecture 2, Imitation Learning","description":"UC Berkeley의 CS 285 강의인 Deep Reinforcement Learning의 Lecture 2 정리","keywords":["Deep Reinforcement Learning"],"articleBody":" 강의 자료: UC Berkeley - Deep Reinforcement Learning\n강의 영상: Youtube Link\nTerminology \u0026 notation\n$\\mathbf{s}_t$: state\n$\\mathbf{o}_t$: observation\n$\\mathbf{a}_t$: action\n$\\pi_\\theta(\\mathbf{a}_t | \\mathbf{o}_t)$: policy\n$\\pi_\\theta(\\mathbf{a}_t | \\mathbf{s}_t)$: policy (fully observed)\n$s_t$와 $o_t$는 별개이다. 단, Imitation Learning에서는 대부분 같다.\n$o_t$에서 $s_t$ 를 완전히 추론하지 못하는 경우도 있다. 예를 들어, 아래 이미지에서 치타를 추론하고자 한다. 자동차에 가려져 있어 치타를 추론하지 못할 수 있다.\n반면, $s_t$에서 $o_t$로는 항상 계산 가능하다.\nMarkov property\n미래는 현재에 따라 결정된다. 과거와 무관하다. 강화 학습에서의 표기법과 로봇\u0026제어에서의 표기법\n강화 학습 로봇\u0026제어 state $\\mathbf{s}_t$ $\\mathbf{x}_t$ action $\\mathbf{a}_t$ $\\mathbf{u}_t$ 학자 Richard Bellman(동적 프로그래밍(Dynamic Programming) 개발자) Lev Pontryagin(최적 제어 이론(Optimal Control Theory)의 선구자) Behavioral cloning이 잘 안 되는 이유\n근본적인 이유는 i.i.d. property 때문이다. 학습 시 $\\mathbf{o}t$ 에서의 라벨링은 $\\mathbf{o}{t+1}$ 에 영향을 미치지 않는다. 그러나 현실에서는 과거의 선택이 미래의 Observation에 영향을 미치기 때문에 결과적으로 제대로 동작하지 않는다. Images: Bojarski et al. ‘16, NVIDIA\nstate를 1차원으로 표현할 수 없지만 직관적인 설명을 위해 가정하였다.\nBehavioral cloning이 잘 안 되는 이유의 이론적 배경\nThe distributional shift problem train dataset의 확률 분포와 최종 정책 확률 분포의 차이로 인해 제대로 동작하지 않는다. 수학적 분석을 위해 몇 가지 가정을 한다.\n학습된 정책 확률 분포의 좋고 나쁨을 판단하기 위해 cost function(reward function)을 정의한다.\n$c(\\mathbf{s}_t,\\mathbf{a}_t) = \\begin{cases} 0 \u0026 \\text{if } \\mathbf{a}_t = \\pi^(\\mathbf{s}) \\ 1 \u0026 \\text{otherwise} \\end{cases}$ ($\\pi^(\\mathbf{s})$은 운전자의 행동이 최적이라고 가정)\n학습의 목적을 cost function의 최대화로 설정한다.\n$\\text{Goal: minimize } \\mathbb{E}{\\mathbf{s}t \\sim p{\\pi\\theta}(\\mathbf{s}_t)} \\left[ c(\\mathbf{s}_t, \\mathbf{a}_t) \\right]$\n작은 실수 확률 $\\epsilon$ 정의: train data set에 등장하는 상태 $\\mathbf{s}$에서는 운전자의 행동 $\\pi^*(s)$와 다른 행동을 할 확률이 $\\epsilon$ 이하라고 가정한다. $\\epsilon$은 작지만 0이 아닌 확률이기에 ‘실수가 발생할 수 있을’을 의미한다.\n$\\text{assume}: \\pi_\\theta(\\mathbf{a} \\neq \\pi^*(\\mathbf{s})|\\mathbf{s}) \\leq \\epsilon \\\\text{for all } \\mathbf{s} \\in \\mathcal{D}_{train}$\n$T$에 대한 cost 계산\n$E[\\sum_t{c(\\mathbf{s}_t, \\mathbf{a}_t)}] \\leq \\epsilon T + (1 - \\epsilon)(\\epsilon (T - 1) + (1- \\epsilon)(\\ldots))$\n→ $O(\\epsilon T^2)$\n💡계산 방식\r첫 스템에서 실수할 확률은 $\\epsilon$ 이다. 이후 나머지 스템 $T - 1$ 전체가 실수한다고 생각하면, 기여 비용(Contribution to cost)는 $\\epsilon \\times T$ 이다. 첫 스텝에 실수가 없을 확률은 $(1 - \\epsilon)$이다. 두 번째 스텝에서 실수할 확률은 $\\epsilon$ 이다. 두 번째 스텝부터 실수한다고 생각하면, $T - 1$ 스텝 동안 실수하는 것이다. 이때 기여 비용은 $(1 - \\epsilon)\\epsilon \\ \\times \\ (T - 1) \\approx \\epsilon T$ 즉, 전체 $T$에 대한 cost를 계산하면 $O(\\epsilon T^2)$임을 알 수 있다. $O(\\epsilon T^2)$에 대한 의미\n작은 실수 확률 $\\epsilon$이 존재해도 $T$가 길어지면, 최대 $O(\\epsilon T^2)$이라는 큰 누적 cost가 발생할 수 있다. 극단적인 예시\n극단적인 예시로 외줄 타기를 들 수 있다. 외줄 타기에서 한 번만 실수해도 바닥으로 떨어진다. 즉, 실수 한 번 이후 상태는 전부 ‘학습이 안 되어 있는 상태’가 된다. 남은 시간 동안 실수가 누적될 수 있다. 확률 분포 관점에서의 분석 $(1 - \\epsilon)^t$: 한 번도 실수하지 않을 확률 $p_{mistake}(\\mathbf{s}_t)$: 조금이라도 실수를 한 뒤에 가게 되는 분포 (train data set에서 벗어난 상태) Total Variation Distance(TV Norm)를 활용한 수식 변형\nTV Norm은 두 분포는 얼마나 차이나는가?를 판단한다. 두 분포의 거리의 최댓값을 나타내며, Norm이 0에 가까워지면 두 분포가 근사적으로 일치함을 의미한다.\n학습된 분포와 train data set 사이의 차이를 확인한다. $\\begin{aligned} \\left| p_\\theta(\\mathbf{s}t) - p{\\text{train}}(\\mathbf{s}t) \\right| \u0026= |(1 - \\epsilon)^t p{\\text{train}}(\\mathbf{s}t) + (1 - (1 - \\epsilon)^t)p{\\text{mistake}}(\\mathbf{s}t) - p{\\text{train}}(\\mathbf{s}t)| \\ \u0026= |-(1 - (1 - \\epsilon)^t)p{\\text{train}}(\\mathbf{s}t) + (1 - (1 - \\epsilon)^t)p{\\text{mistake}}(\\mathbf{s}t)| \\ \u0026= |(1 - (1 - \\epsilon)^t)(p{\\text{mistake}}(\\mathbf{s}t) - p{\\text{train}}(\\mathbf{s}t))| \\ \u0026= (1 - (1 - \\epsilon)^t) \\left| p{\\text{mistake}}(\\mathbf{s}t) - p{\\text{train}}(\\mathbf{s}_t) \\right| \\end{aligned}$\n$(1 - \\epsilon)^t \\geq 1 - \\epsilon t \\text{ for } \\epsilon \\in [0, 1]$ 임을 활용하여 $1-(1-\\epsilon)^t \\approx \\epsilon t$를 계산한다.\n$\\begin{aligned} (1-\\epsilon)^t \u0026\\geq 1 - \\epsilon t \\ \u0026\\Rightarrow -1+(1-\\epsilon)^t \\geq - \\epsilon t \\ \u0026\\Rightarrow 1 - (1-\\epsilon)^t \\leq \\epsilon t \\ \u0026\\therefore 1-(1-\\epsilon)^t \\approx \\epsilon t \\end{aligned}$\n즉, 아래와 같이 TV Norm을 정리할 수 있다.\n$\\begin{aligned} \\left| p_\\theta(\\mathbf{s}t) - p{\\text{train}}(\\mathbf{s}t) \\right| = (1 - (1 - \\epsilon)^t) \\left| p{\\text{mistake}}(\\mathbf{s}t) - p{\\text{train}}(\\mathbf{s}_t) \\right| \u0026\\leq 2(1 - (1-\\epsilon)^t) \\\u0026\\leq 2 \\epsilon t \\end{aligned}$\n2배인 직관적인 이유\n$p_\\theta(\\mathbf{s}), p_{train}(\\mathbf{s})$은 확률 분포이므로 최솟값은 0, 최댓값은 1이다.\n$0 \\leq p_\\theta(\\mathbf{s}) \\leq 1, \\ 0 \\leq p_{train}(\\mathbf{s}) \\leq 1$\n단일 상태일 경우 $\\left| p_\\theta(\\mathbf{s}t) - p{\\text{train}}(\\mathbf{s}_t) \\right|$의 최댓값은 1이다. (한쪽이 1, 다른 쪽이 0)\n그러나 분포 간의 전체 차이를 확인하자면 최댓값은 2이다.\n$\\sum_s p_\\theta(\\mathbf{s}) = 1, \\ \\sum_s p_{train}(\\mathbf{s}) = 1$\n분포 간에 전혀 겹치는 부분이 없는 경우를 가정하면 아래와 같다.\n$p_\\theta(s_0) = 1, \\ p_\\theta(s_1) = 0$\n$p_{train}(s_0) = 0, \\ p_{train}(s_1) = 1$\n$\\sum_s \\left| p_\\theta(\\mathbf{s}) - p_{\\text{train}}(\\mathbf{s}) \\right| = \\left| 1 - 0 \\right| + \\left| 0 - 1 \\right| = 2$\n2배 뒤에 $(1 - (1-\\epsilon)^t)$인 이유\n양변의 공통 계수를 맞추기 위해\n시간 축에 대하여 cost를 모두 합산했을 때 $\\sum_t \\epsilon + 2 \\epsilon t$를 계산할 수 있다.\n$\\begin{aligned} \\sum_t \\epsilon + 2 \\epsilon t \u0026= \\epsilon \\sum_t^T t + 2 \\epsilon t \\ \u0026= \\epsilon \\frac{T(T+2)}{2} + 2 \\epsilon t \\\u0026= \\epsilon(\\frac{T(T+2)}{2} + 2t) \\\u0026\\text{if T가 아주 크다면,} \\\u0026\\approx O(\\epsilon T^2) \\end{aligned}$\n결론\n$T$가 커질 수록 $\\epsilon$이 작아도 누적 cost가 엄청 커진다. Behavior cloning을 사용했을 때, train에서 벗어난 상황이 발생하면 대응하지 못하는 경우가 발생할 수 있음을 강조한다. Imitation Learning은 train data에 적당한 실수와 오류가 있어야 더 잘 동작한다.\nbehavior cloning을 활용한 Imitation learning을 잘 동작하게 하는 방법\ncollect and augment 의도적으로 실수(mistakes)와 수정 사항(corrections)을 추가한다. 실수를 학습을 방해하지만 수정 사항은 도와준다. 그러나 수정 사항은 종종 실수 보다 크게 방해할 수 있다. 데이터 증강(data augmentation) Case Study trail following as classification - GIUSTI, Alessandro, et al. A machine learning approach to visual perception of forest trails for mobile robots. IEEE Robotics and Automation Letters, 2015, 1.2: 661-667. imitation with a cheap robot - RAHMATIZADEH, Rouhollah, et al. Vision-based multi-task manipulation for inexpensive robots using end-to-end learning from demonstration. In: 2018 IEEE international conference on robotics and automation (ICRA). IEEE, 2018. p. 3758-3765. 강력한 모델 사용하기 Non-Markovian behavior\nsequence model을 활용하여 전체 기록을 학습한다. Multimodeal behavior\nexpressive continuous distribution을 사용한다. mixture of Gaussians 장점: 구현하기 쉬움 단점: weight를 지정해야 함. latent variable models 해당 분야에서 널리 사용되는 variational sutoencoder 모델 사용하기 diffusion models 고차원 액션 스페이스를 이산화한다. 이산화의 문제: 1차원은 쉬우나 고차원은 어렵다. 해결 방안: 한 번에 한 차원씩 이산화하기 Autoregressive discretization sequence model block을 활용하여 이산화하기 장점: 복잡한 분포를 얻을 수 있다. 단점: 시퀀스 모델을 사용하여 무겁다. Study Case imitation with diffustion models - CHI, Cheng, et al. Diffusion policy: Visuomotor policy learning via action diffusion. The International Journal of Robotics Research, 2023, 02783649241273668. imitation with latent variables - ZHAO, Tony Z., et al. Learning fine-grained bimanual manipulation with low-cost hardware. arXiv preprint arXiv:2304.13705, 2023. imitation with transformers - BROHAN, Anthony, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022. Multi-task learning\n어떻게 두 번째 장소에 도착한 것을 알 수 있을까? 아직은 잘 모르겠지만 상당히 신기한 부분이다. Study Case YU, Tianhe, et al. Unsupervised visuomotor control through distributional planning networks. arXiv preprint arXiv:1902.05542, 2019. GHOSH, Dibya, et al. Learning to reach goals via iterated supervised learning. arXiv preprint arXiv:1912.06088, 2019. ANDRYCHOWICZ, Marcin, et al. Hindsight experience replay. Advances in neural information processing systems, 2017, 30. ","wordCount":"1137","inLanguage":"en","image":"https://son-dongwoo.github.io/posts/lecture-2/images/cover.webp","datePublished":"2025-02-05T01:10:35+09:00","dateModified":"2025-02-05T01:10:35+09:00","author":{"@type":"Person","name":"Son-Dongwoo"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://son-dongwoo.github.io/posts/lecture-2/"},"publisher":{"@type":"Organization","name":"Son-Dongwoo's Blog","logo":{"@type":"ImageObject","url":"https://son-dongwoo.github.io/%3Clink%20/%20abs%20url%3E"}}}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})'></script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://son-dongwoo.github.io/ accesskey=h title="Son-Dongwoo's Blog (Alt + H)"><img src=https://son-dongwoo.github.io/apple-touch-icon.png alt aria-label=logo height=35>Son-Dongwoo's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://son-dongwoo.github.io/posts/ title="📝 posts"><span>📝 posts</span></a></li><li><a href=https://son-dongwoo.github.io/projects/ title="🚀 projects"><span>🚀 projects</span></a></li><li><a href=https://son-dongwoo.github.io/tags/ title="🏷️ tags"><span>🏷️ tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://son-dongwoo.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://son-dongwoo.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Lecture 2, Imitation Learning</h1><div class=post-description>UC Berkeley의 CS 285 강의인 Deep Reinforcement Learning의 Lecture 2 정리</div><div class=post-meta><span title='2025-02-05 01:10:35 +0900 KST'>February 5, 2025</span>&nbsp;·&nbsp;Son-Dongwoo</div><ul class=post-tags><li><a href=https://son-dongwoo.github.io/tags/deep-reinforcement-learning/>🏷️Deep Reinforcement Learning</a></li></ul></header><div class=post-content><blockquote><p>강의 자료: <a href=https://rail.eecs.berkeley.edu/deeprlcourse/>UC Berkeley - Deep Reinforcement Learning</a><br>강의 영상: <a href="https://www.youtube.com/playlist?list=PL_iWQOsE6TfVYGEGiAOMaOzzv41Jfm_Ps">Youtube Link</a></p></blockquote><ul><li><p>Terminology & notation</p><ul><li><p>$\mathbf{s}_t$: state</p></li><li><p>$\mathbf{o}_t$: observation</p></li><li><p>$\mathbf{a}_t$: action</p></li><li><p>$\pi_\theta(\mathbf{a}_t | \mathbf{o}_t)$: policy</p></li><li><p>$\pi_\theta(\mathbf{a}_t | \mathbf{s}_t)$: policy (fully observed)</p></li><li><p>$s_t$와 $o_t$는 별개이다. 단, Imitation Learning에서는 대부분 같다.</p></li><li><p>$o_t$에서 $s_t$ 를 완전히 추론하지 못하는 경우도 있다. 예를 들어, 아래 이미지에서 치타를 추론하고자 한다. 자동차에 가려져 있어 치타를 추론하지 못할 수 있다.</p><p><img alt=image-1.jpg loading=lazy src=/posts/lecture-2/images/image-1.png></p></li><li><p>반면, $s_t$에서 $o_t$로는 항상 계산 가능하다.</p></li></ul></li><li><p>Markov property</p><ul><li>미래는 현재에 따라 결정된다. 과거와 무관하다.</li></ul></li><li><p>강화 학습에서의 표기법과 로봇&제어에서의 표기법</p><table><thead><tr><th></th><th>강화 학습</th><th>로봇&제어</th></tr></thead><tbody><tr><td>state</td><td>$\mathbf{s}_t$</td><td>$\mathbf{x}_t$</td></tr><tr><td>action</td><td>$\mathbf{a}_t$</td><td>$\mathbf{u}_t$</td></tr><tr><td>학자</td><td>Richard Bellman(동적 프로그래밍(Dynamic Programming) 개발자)</td><td>Lev Pontryagin(최적 제어 이론(Optimal Control Theory)의 선구자)</td></tr></tbody></table></li><li><p>Behavioral cloning이 잘 안 되는 이유</p><ul><li>근본적인 이유는 <a href="https://www.notion.so/Lecture-1-Introduction-180c8c7ad23c80d883f6f8acaf190bff?pvs=21">i.i.d. property</a> 때문이다.</li><li>학습 시 $\mathbf{o}<em>t$ 에서의 라벨링은 $\mathbf{o}</em>{t+1}$ 에 영향을 미치지 않는다. 그러나 현실에서는 과거의 선택이 미래의 Observation에 영향을 미치기 때문에 결과적으로 제대로 동작하지 않는다.</li></ul><p><img alt="Images: Bojarski et al. ‘16, NVIDIA" loading=lazy src=/posts/lecture-2/images/image-2.png></p><p>Images: Bojarski et al. ‘16, NVIDIA</p><p><img alt="state를 1차원으로 표현할 수 없지만 직관적인 설명을 위해 가정하였다." loading=lazy src=/posts/lecture-2/images/image-3.png></p><p>state를 1차원으로 표현할 수 없지만 직관적인 설명을 위해 가정하였다.</p></li><li><p>Behavioral cloning이 잘 안 되는 이유의 이론적 배경</p><ul><li>The distributional shift problem<ul><li>train dataset의 확률 분포와 최종 정책 확률 분포의 차이로 인해 제대로 동작하지 않는다.</li></ul></li></ul><p><img alt=image.jpg loading=lazy src=/posts/lecture-2/images/image-4.png></p><ul><li><p>수학적 분석을 위해 몇 가지 가정을 한다.</p><ol><li><p>학습된 정책 확률 분포의 좋고 나쁨을 판단하기 위해 cost function(reward function)을 정의한다.</p><p>$c(\mathbf{s}_t,\mathbf{a}_t) =
\begin{cases}
0 & \text{if } \mathbf{a}_t = \pi^<em>(\mathbf{s}) \
1 & \text{otherwise}
\end{cases}$
($\pi^</em>(\mathbf{s})$은 운전자의 행동이 최적이라고 가정)</p></li><li><p>학습의 목적을 cost function의 최대화로 설정한다.</p><p>$\text{Goal: minimize } \mathbb{E}<em>{\mathbf{s}<em>t \sim p</em>{\pi</em>\theta}(\mathbf{s}_t)} \left[ c(\mathbf{s}_t, \mathbf{a}_t) \right]$</p></li><li><p>작은 실수 확률 $\epsilon$ 정의: train data set에 등장하는 상태 $\mathbf{s}$에서는 운전자의 행동 $\pi^*(s)$와 다른 행동을 할 확률이 $\epsilon$ 이하라고 가정한다. $\epsilon$은 작지만 0이 아닌 확률이기에 ‘실수가 발생할 수 있을’을 의미한다.</p><p>$\text{assume}: \pi_\theta(\mathbf{a} \neq \pi^*(\mathbf{s})|\mathbf{s}) \leq \epsilon
\\text{for all } \mathbf{s} \in \mathcal{D}_{train}$</p></li></ol></li><li><p>$T$에 대한 cost 계산</p><p>$E[\sum_t{c(\mathbf{s}_t, \mathbf{a}_t)}] \leq \epsilon T + (1 - \epsilon)(\epsilon (T - 1) + (1- \epsilon)(\ldots))$</p><p>→ $O(\epsilon T^2)$</p></li></ul></li></ul><aside style="border-radius:var(--radius);background:var(--code-bg);padding:5px;border-left:5px solid #f1c40f">💡계산 방식<ol><li>첫 스템에서 실수할 확률은 $\epsilon$ 이다. 이후 나머지 스템 $T - 1$ 전체가 실수한다고 생각하면, 기여 비용(Contribution to cost)는 $\epsilon \times T$ 이다.</li><li>첫 스텝에 실수가 없을 확률은 $(1 - \epsilon)$이다. 두 번째 스텝에서 실수할 확률은 $\epsilon$ 이다. 두 번째 스텝부터 실수한다고 생각하면, $T - 1$ 스텝 동안 실수하는 것이다. 이때 기여 비용은 $(1 - \epsilon)\epsilon \ \times \ (T - 1) \approx \epsilon T$</li><li>즉, 전체 $T$에 대한 cost를 계산하면 $O(\epsilon T^2)$임을 알 수 있다.</li></ol></aside><ul><li><p>$O(\epsilon T^2)$에 대한 의미</p><ul><li>작은 실수 확률 $\epsilon$이 존재해도 $T$가 길어지면, 최대 $O(\epsilon T^2)$이라는 큰 누적 cost가 발생할 수 있다.</li></ul></li><li><p>극단적인 예시</p><ul><li>극단적인 예시로 외줄 타기를 들 수 있다.</li><li>외줄 타기에서 한 번만 실수해도 바닥으로 떨어진다. 즉, 실수 한 번 이후 상태는 전부 ‘학습이 안 되어 있는 상태’가 된다. 남은 시간 동안 실수가 누적될 수 있다.</li></ul></li></ul><div style=display:flex;justify-content:space-between><img src=images/image-5.png style=width:48%>
<img src=images/image-6.png style=width:48%></div><ul><li>확률 분포 관점에서의 분석<ul><li>$(1 - \epsilon)^t$: 한 번도 실수하지 않을 확률</li><li>$p_{mistake}(\mathbf{s}_t)$: 조금이라도 실수를 한 뒤에 가게 되는 분포 (train data set에서 벗어난 상태)</li></ul></li></ul><p><img alt=image-7.jpg loading=lazy src=/posts/lecture-2/images/image-7.jpg></p><ul><li><p><a href=https://blog.naver.com/ycpiglet/223087981574>Total Variation Distance(TV Norm)</a>를 활용한 수식 변형</p><ul><li><p>TV Norm은 두 분포는 얼마나 차이나는가?를 판단한다. 두 분포의 거리의 최댓값을 나타내며, Norm이 0에 가까워지면 두 분포가 근사적으로 일치함을 의미한다.</p></li><li><p>학습된 분포와 train data set 사이의 차이를 확인한다.
$\begin{aligned}
\left| p_\theta(\mathbf{s}<em>t) - p</em>{\text{train}}(\mathbf{s}<em>t) \right|
&= |(1 - \epsilon)^t p</em>{\text{train}}(\mathbf{s}<em>t) + (1 - (1 - \epsilon)^t)p</em>{\text{mistake}}(\mathbf{s}<em>t) - p</em>{\text{train}}(\mathbf{s}<em>t)| \
&= |-(1 - (1 - \epsilon)^t)p</em>{\text{train}}(\mathbf{s}<em>t) + (1 - (1 - \epsilon)^t)p</em>{\text{mistake}}(\mathbf{s}<em>t)| \
&= |(1 - (1 - \epsilon)^t)(p</em>{\text{mistake}}(\mathbf{s}<em>t) - p</em>{\text{train}}(\mathbf{s}<em>t))| \
&= (1 - (1 - \epsilon)^t) \left| p</em>{\text{mistake}}(\mathbf{s}<em>t) - p</em>{\text{train}}(\mathbf{s}_t) \right|
\end{aligned}$</p></li><li><p>$(1 - \epsilon)^t \geq 1 - \epsilon t \text{ for } \epsilon \in [0, 1]$ 임을 활용하여 $1-(1-\epsilon)^t \approx \epsilon t$를 계산한다.</p><p>$\begin{aligned}
(1-\epsilon)^t &\geq 1 - \epsilon t \
&\Rightarrow -1+(1-\epsilon)^t \geq - \epsilon t \
&\Rightarrow 1 - (1-\epsilon)^t \leq \epsilon t \
&\therefore 1-(1-\epsilon)^t \approx \epsilon t
\end{aligned}$</p></li><li><p>즉, 아래와 같이 TV Norm을 정리할 수 있다.</p><p>$\begin{aligned}
\left| p_\theta(\mathbf{s}<em>t) - p</em>{\text{train}}(\mathbf{s}<em>t) \right| = (1 - (1 - \epsilon)^t) \left| p</em>{\text{mistake}}(\mathbf{s}<em>t) - p</em>{\text{train}}(\mathbf{s}_t) \right| &\leq 2(1 - (1-\epsilon)^t) \&\leq 2 \epsilon t
\end{aligned}$</p></li><li><p>2배인 직관적인 이유</p><ul><li><p>$p_\theta(\mathbf{s}), p_{train}(\mathbf{s})$은 확률 분포이므로 최솟값은 0, 최댓값은 1이다.</p><p>$0 \leq p_\theta(\mathbf{s}) \leq 1, \ 0 \leq p_{train}(\mathbf{s}) \leq 1$</p><p>단일 상태일 경우 $\left| p_\theta(\mathbf{s}<em>t) - p</em>{\text{train}}(\mathbf{s}_t) \right|$의 최댓값은 1이다. (한쪽이 1, 다른 쪽이 0)</p></li><li><p>그러나 분포 간의 전체 차이를 확인하자면 최댓값은 2이다.</p><p>$\sum_s p_\theta(\mathbf{s}) = 1, \ \sum_s p_{train}(\mathbf{s}) = 1$</p></li><li><p>분포 간에 전혀 겹치는 부분이 없는 경우를 가정하면 아래와 같다.</p><p>$p_\theta(s_0) = 1, \ p_\theta(s_1) = 0$</p><p>$p_{train}(s_0) = 0, \ p_{train}(s_1) = 1$</p><p>$\sum_s \left| p_\theta(\mathbf{s}) - p_{\text{train}}(\mathbf{s}) \right| = \left| 1 - 0 \right| + \left| 0 - 1 \right| = 2$</p></li></ul></li><li><p>2배 뒤에 $(1 - (1-\epsilon)^t)$인 이유</p><p>양변의 공통 계수를 맞추기 위해</p></li><li><p>시간 축에 대하여 cost를 모두 합산했을 때 $\sum_t \epsilon + 2 \epsilon t$를 계산할 수 있다.</p><p>$\begin{aligned}
\sum_t \epsilon + 2 \epsilon t &= \epsilon \sum_t^T t + 2 \epsilon t \
&= \epsilon \frac{T(T+2)}{2} + 2 \epsilon t
\&= \epsilon(\frac{T(T+2)}{2} + 2t)
\&\text{if T가 아주 크다면,}
\&\approx O(\epsilon T^2)
\end{aligned}$</p></li></ul></li><li><p>결론</p><ul><li>$T$가 커질 수록 $\epsilon$이 작아도 누적 cost가 엄청 커진다.</li><li>Behavior cloning을 사용했을 때, train에서 벗어난 상황이 발생하면 대응하지 못하는 경우가 발생할 수 있음을 강조한다.</li></ul></li><li><p>Imitation Learning은 train data에 적당한 실수와 오류가 있어야 더 잘 동작한다.</p></li><li><p>behavior cloning을 활용한 Imitation learning을 잘 동작하게 하는 방법</p><ol><li>collect and augment<ul><li>의도적으로 실수(mistakes)와 수정 사항(corrections)을 추가한다.<ul><li>실수를 학습을 방해하지만 수정 사항은 도와준다. 그러나 수정 사항은 종종 실수 보다 크게 방해할 수 있다.</li></ul></li><li>데이터 증강(data augmentation)</li><li>Case Study<ol><li>trail following as classification - GIUSTI, Alessandro, et al. A machine learning approach to visual perception of forest trails for mobile robots. <em>IEEE Robotics and Automation Letters</em>, 2015, 1.2: 661-667.</li><li>imitation with a cheap robot - RAHMATIZADEH, Rouhollah, et al. Vision-based multi-task manipulation for inexpensive robots using end-to-end learning from demonstration. In: <em>2018 IEEE international conference on robotics and automation (ICRA)</em>. IEEE, 2018. p. 3758-3765.</li></ol></li></ul></li><li>강력한 모델 사용하기<ul><li><p>Non-Markovian behavior</p><ul><li>sequence model을 활용하여 전체 기록을 학습한다.</li></ul><p><img alt=image-8.jpg loading=lazy src=/posts/lecture-2/images/image-8.jpg></p></li><li><p>Multimodeal behavior</p><ol><li>expressive continuous distribution을 사용한다.<ol><li>mixture of Gaussians<ol><li>장점: 구현하기 쉬움</li><li>단점: weight를 지정해야 함.</li></ol></li><li>latent variable models<ol><li>해당 분야에서 널리 사용되는 variational sutoencoder 모델 사용하기</li></ol></li><li>diffusion models</li></ol></li><li>고차원 액션 스페이스를 이산화한다.<ol><li>이산화의 문제: 1차원은 쉬우나 고차원은 어렵다.</li><li>해결 방안: 한 번에 한 차원씩 이산화하기<ol><li>Autoregressive discretization<ol><li>sequence model block을 활용하여 이산화하기<ol><li>장점: 복잡한 분포를 얻을 수 있다.</li><li>단점: 시퀀스 모델을 사용하여 무겁다.</li></ol></li></ol></li></ol></li></ol></li><li>Study Case<ol><li>imitation with diffustion models - CHI, Cheng, et al. Diffusion policy: Visuomotor policy learning via action diffusion. <em>The International Journal of Robotics Research</em>, 2023, 02783649241273668.</li><li>imitation with latent variables - ZHAO, Tony Z., et al. Learning fine-grained bimanual manipulation with low-cost hardware. <em>arXiv preprint arXiv:2304.13705</em>, 2023.</li><li>imitation with transformers - BROHAN, Anthony, et al. Rt-1: Robotics transformer for real-world control at scale. <em>arXiv preprint arXiv:2212.06817</em>, 2022.</li></ol></li></ol></li><li><p>Multi-task learning</p><p><img alt=image-9.jpg loading=lazy src=/posts/lecture-2/images/image-9.jpg></p><p><img alt=image-10.jpg loading=lazy src=/posts/lecture-2/images/image-10.jpg></p><ul><li>어떻게 두 번째 장소에 도착한 것을 알 수 있을까? 아직은 잘 모르겠지만 상당히 신기한 부분이다.</li><li>Study Case<ul><li>YU, Tianhe, et al. Unsupervised visuomotor control through distributional planning networks. <em>arXiv preprint arXiv:1902.05542</em>, 2019.</li><li>GHOSH, Dibya, et al. Learning to reach goals via iterated supervised learning. <em>arXiv preprint arXiv:1912.06088</em>, 2019.</li><li>ANDRYCHOWICZ, Marcin, et al. <strong>Hindsight experience replay</strong>. <em>Advances in neural information processing systems</em>, 2017, 30.</li></ul></li></ul></li></ul></li></ol></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://son-dongwoo.github.io/tags/deep-reinforcement-learning/>🏷️Deep Reinforcement Learning</a></li></ul><nav class=paginav><a class=next href=https://son-dongwoo.github.io/posts/lecture-1/><span class=title>Next »</span><br><span>Lecture 1, Introduction</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://son-dongwoo.github.io/>Son-Dongwoo's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>