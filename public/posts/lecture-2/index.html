<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Lecture 2, Imitation Learning | Son-Dongwoo's Blog</title>
<meta name=keywords content="Deep Reinforcement Learning"><meta name=description content="UC Berkeleyì˜ CS 285 ê°•ì˜ì¸ Deep Reinforcement Learningì˜ Lecture 2 ì •ë¦¬"><meta name=author content="Son-Dongwoo"><link rel=canonical href=https://son-dongwoo.github.io/posts/lecture-2/><meta name=google-site-verification content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://son-dongwoo.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://son-dongwoo.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://son-dongwoo.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://son-dongwoo.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://son-dongwoo.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://son-dongwoo.github.io/posts/lecture-2/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:url" content="https://son-dongwoo.github.io/posts/lecture-2/"><meta property="og:site_name" content="Son-Dongwoo's Blog"><meta property="og:title" content="Lecture 2, Imitation Learning"><meta property="og:description" content="UC Berkeleyì˜ CS 285 ê°•ì˜ì¸ Deep Reinforcement Learningì˜ Lecture 2 ì •ë¦¬"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-02-05T01:10:35+09:00"><meta property="article:modified_time" content="2025-02-05T01:10:35+09:00"><meta property="article:tag" content="Deep Reinforcement Learning"><meta property="og:image" content="https://son-dongwoo.github.io/posts/lecture-2/images/cover.webp"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://son-dongwoo.github.io/posts/lecture-2/images/cover.webp"><meta name=twitter:title content="Lecture 2, Imitation Learning"><meta name=twitter:description content="UC Berkeleyì˜ CS 285 ê°•ì˜ì¸ Deep Reinforcement Learningì˜ Lecture 2 ì •ë¦¬"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://son-dongwoo.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Lecture 2, Imitation Learning","item":"https://son-dongwoo.github.io/posts/lecture-2/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Lecture 2, Imitation Learning","name":"Lecture 2, Imitation Learning","description":"UC Berkeleyì˜ CS 285 ê°•ì˜ì¸ Deep Reinforcement Learningì˜ Lecture 2 ì •ë¦¬","keywords":["Deep Reinforcement Learning"],"articleBody":" ê°•ì˜ ìë£Œ: UC Berkeley - Deep Reinforcement Learning\nê°•ì˜ ì˜ìƒ: Youtube Link\nTerminology \u0026 notation\n$\\mathbf{s}_t$: state\n$\\mathbf{o}_t$: observation\n$\\mathbf{a}_t$: action\n$\\pi_\\theta(\\mathbf{a}_t | \\mathbf{o}_t)$: policy\n$\\pi_\\theta(\\mathbf{a}_t | \\mathbf{s}_t)$: policy (fully observed)\n$s_t$ì™€ $o_t$ëŠ” ë³„ê°œì´ë‹¤. ë‹¨, Imitation Learningì—ì„œëŠ” ëŒ€ë¶€ë¶„ ê°™ë‹¤.\n$o_t$ì—ì„œ $s_t$ ë¥¼ ì™„ì „íˆ ì¶”ë¡ í•˜ì§€ ëª»í•˜ëŠ” ê²½ìš°ë„ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì•„ë˜ ì´ë¯¸ì§€ì—ì„œ ì¹˜íƒ€ë¥¼ ì¶”ë¡ í•˜ê³ ì í•œë‹¤. ìë™ì°¨ì— ê°€ë ¤ì ¸ ìˆì–´ ì¹˜íƒ€ë¥¼ ì¶”ë¡ í•˜ì§€ ëª»í•  ìˆ˜ ìˆë‹¤.\në°˜ë©´, $s_t$ì—ì„œ $o_t$ë¡œëŠ” í•­ìƒ ê³„ì‚° ê°€ëŠ¥í•˜ë‹¤.\nMarkov property\në¯¸ë˜ëŠ” í˜„ì¬ì— ë”°ë¼ ê²°ì •ëœë‹¤. ê³¼ê±°ì™€ ë¬´ê´€í•˜ë‹¤. ê°•í™” í•™ìŠµì—ì„œì˜ í‘œê¸°ë²•ê³¼ ë¡œë´‡\u0026ì œì–´ì—ì„œì˜ í‘œê¸°ë²•\nê°•í™” í•™ìŠµ ë¡œë´‡\u0026ì œì–´ state $\\mathbf{s}_t$ $\\mathbf{x}_t$ action $\\mathbf{a}_t$ $\\mathbf{u}_t$ í•™ì Richard Bellman(ë™ì  í”„ë¡œê·¸ë˜ë°(Dynamic Programming) ê°œë°œì) Lev Pontryagin(ìµœì  ì œì–´ ì´ë¡ (Optimal Control Theory)ì˜ ì„ êµ¬ì) Behavioral cloningì´ ì˜ ì•ˆ ë˜ëŠ” ì´ìœ \nê·¼ë³¸ì ì¸ ì´ìœ ëŠ” i.i.d. property ë•Œë¬¸ì´ë‹¤. í•™ìŠµ ì‹œ $\\mathbf{o}t$ ì—ì„œì˜ ë¼ë²¨ë§ì€ $\\mathbf{o}{t+1}$ ì— ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•ŠëŠ”ë‹¤. ê·¸ëŸ¬ë‚˜ í˜„ì‹¤ì—ì„œëŠ” ê³¼ê±°ì˜ ì„ íƒì´ ë¯¸ë˜ì˜ Observationì— ì˜í–¥ì„ ë¯¸ì¹˜ê¸° ë•Œë¬¸ì— ê²°ê³¼ì ìœ¼ë¡œ ì œëŒ€ë¡œ ë™ì‘í•˜ì§€ ì•ŠëŠ”ë‹¤. Images: Bojarski et al. â€˜16, NVIDIA\nstateë¥¼ 1ì°¨ì›ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ì—†ì§€ë§Œ ì§ê´€ì ì¸ ì„¤ëª…ì„ ìœ„í•´ ê°€ì •í•˜ì˜€ë‹¤.\nBehavioral cloningì´ ì˜ ì•ˆ ë˜ëŠ” ì´ìœ ì˜ ì´ë¡ ì  ë°°ê²½\nThe distributional shift problem train datasetì˜ í™•ë¥  ë¶„í¬ì™€ ìµœì¢… ì •ì±… í™•ë¥  ë¶„í¬ì˜ ì°¨ì´ë¡œ ì¸í•´ ì œëŒ€ë¡œ ë™ì‘í•˜ì§€ ì•ŠëŠ”ë‹¤. ìˆ˜í•™ì  ë¶„ì„ì„ ìœ„í•´ ëª‡ ê°€ì§€ ê°€ì •ì„ í•œë‹¤.\ní•™ìŠµëœ ì •ì±… í™•ë¥  ë¶„í¬ì˜ ì¢‹ê³  ë‚˜ì¨ì„ íŒë‹¨í•˜ê¸° ìœ„í•´ cost function(reward function)ì„ ì •ì˜í•œë‹¤.\n$c(\\mathbf{s}_t,\\mathbf{a}_t) = \\begin{cases} 0 \u0026 \\text{if } \\mathbf{a}_t = \\pi^(\\mathbf{s}) \\ 1 \u0026 \\text{otherwise} \\end{cases}$ ($\\pi^(\\mathbf{s})$ì€ ìš´ì „ìì˜ í–‰ë™ì´ ìµœì ì´ë¼ê³  ê°€ì •)\ní•™ìŠµì˜ ëª©ì ì„ cost functionì˜ ìµœëŒ€í™”ë¡œ ì„¤ì •í•œë‹¤.\n$\\text{Goal: minimize } \\mathbb{E}{\\mathbf{s}t \\sim p{\\pi\\theta}(\\mathbf{s}_t)} \\left[ c(\\mathbf{s}_t, \\mathbf{a}_t) \\right]$\nì‘ì€ ì‹¤ìˆ˜ í™•ë¥  $\\epsilon$ ì •ì˜: train data setì— ë“±ì¥í•˜ëŠ” ìƒíƒœ $\\mathbf{s}$ì—ì„œëŠ” ìš´ì „ìì˜ í–‰ë™ $\\pi^*(s)$ì™€ ë‹¤ë¥¸ í–‰ë™ì„ í•  í™•ë¥ ì´ $\\epsilon$ ì´í•˜ë¼ê³  ê°€ì •í•œë‹¤. $\\epsilon$ì€ ì‘ì§€ë§Œ 0ì´ ì•„ë‹Œ í™•ë¥ ì´ê¸°ì— â€˜ì‹¤ìˆ˜ê°€ ë°œìƒí•  ìˆ˜ ìˆì„â€™ì„ ì˜ë¯¸í•œë‹¤.\n$\\text{assume}: \\pi_\\theta(\\mathbf{a} \\neq \\pi^*(\\mathbf{s})|\\mathbf{s}) \\leq \\epsilon \\\\text{for all } \\mathbf{s} \\in \\mathcal{D}_{train}$\n$T$ì— ëŒ€í•œ cost ê³„ì‚°\n$E[\\sum_t{c(\\mathbf{s}_t, \\mathbf{a}_t)}] \\leq \\epsilon T + (1 - \\epsilon)(\\epsilon (T - 1) + (1- \\epsilon)(\\ldots))$\nâ†’ $O(\\epsilon T^2)$\nğŸ’¡ê³„ì‚° ë°©ì‹\rì²« ìŠ¤í…œì—ì„œ ì‹¤ìˆ˜í•  í™•ë¥ ì€ $\\epsilon$ ì´ë‹¤. ì´í›„ ë‚˜ë¨¸ì§€ ìŠ¤í…œ $T - 1$ ì „ì²´ê°€ ì‹¤ìˆ˜í•œë‹¤ê³  ìƒê°í•˜ë©´, ê¸°ì—¬ ë¹„ìš©(Contribution to cost)ëŠ” $\\epsilon \\times T$ ì´ë‹¤. ì²« ìŠ¤í…ì— ì‹¤ìˆ˜ê°€ ì—†ì„ í™•ë¥ ì€ $(1 - \\epsilon)$ì´ë‹¤. ë‘ ë²ˆì§¸ ìŠ¤í…ì—ì„œ ì‹¤ìˆ˜í•  í™•ë¥ ì€ $\\epsilon$ ì´ë‹¤. ë‘ ë²ˆì§¸ ìŠ¤í…ë¶€í„° ì‹¤ìˆ˜í•œë‹¤ê³  ìƒê°í•˜ë©´, $T - 1$ ìŠ¤í… ë™ì•ˆ ì‹¤ìˆ˜í•˜ëŠ” ê²ƒì´ë‹¤. ì´ë•Œ ê¸°ì—¬ ë¹„ìš©ì€ $(1 - \\epsilon)\\epsilon \\ \\times \\ (T - 1) \\approx \\epsilon T$ ì¦‰, ì „ì²´ $T$ì— ëŒ€í•œ costë¥¼ ê³„ì‚°í•˜ë©´ $O(\\epsilon T^2)$ì„ì„ ì•Œ ìˆ˜ ìˆë‹¤. $O(\\epsilon T^2)$ì— ëŒ€í•œ ì˜ë¯¸\nì‘ì€ ì‹¤ìˆ˜ í™•ë¥  $\\epsilon$ì´ ì¡´ì¬í•´ë„ $T$ê°€ ê¸¸ì–´ì§€ë©´, ìµœëŒ€ $O(\\epsilon T^2)$ì´ë¼ëŠ” í° ëˆ„ì  costê°€ ë°œìƒí•  ìˆ˜ ìˆë‹¤. ê·¹ë‹¨ì ì¸ ì˜ˆì‹œ\nê·¹ë‹¨ì ì¸ ì˜ˆì‹œë¡œ ì™¸ì¤„ íƒ€ê¸°ë¥¼ ë“¤ ìˆ˜ ìˆë‹¤. ì™¸ì¤„ íƒ€ê¸°ì—ì„œ í•œ ë²ˆë§Œ ì‹¤ìˆ˜í•´ë„ ë°”ë‹¥ìœ¼ë¡œ ë–¨ì–´ì§„ë‹¤. ì¦‰, ì‹¤ìˆ˜ í•œ ë²ˆ ì´í›„ ìƒíƒœëŠ” ì „ë¶€ â€˜í•™ìŠµì´ ì•ˆ ë˜ì–´ ìˆëŠ” ìƒíƒœâ€™ê°€ ëœë‹¤. ë‚¨ì€ ì‹œê°„ ë™ì•ˆ ì‹¤ìˆ˜ê°€ ëˆ„ì ë  ìˆ˜ ìˆë‹¤. í™•ë¥  ë¶„í¬ ê´€ì ì—ì„œì˜ ë¶„ì„ $(1 - \\epsilon)^t$: í•œ ë²ˆë„ ì‹¤ìˆ˜í•˜ì§€ ì•Šì„ í™•ë¥  $p_{mistake}(\\mathbf{s}_t)$: ì¡°ê¸ˆì´ë¼ë„ ì‹¤ìˆ˜ë¥¼ í•œ ë’¤ì— ê°€ê²Œ ë˜ëŠ” ë¶„í¬ (train data setì—ì„œ ë²—ì–´ë‚œ ìƒíƒœ) Total Variation Distance(TV Norm)ë¥¼ í™œìš©í•œ ìˆ˜ì‹ ë³€í˜•\nTV Normì€ ë‘ ë¶„í¬ëŠ” ì–¼ë§ˆë‚˜ ì°¨ì´ë‚˜ëŠ”ê°€?ë¥¼ íŒë‹¨í•œë‹¤. ë‘ ë¶„í¬ì˜ ê±°ë¦¬ì˜ ìµœëŒ“ê°’ì„ ë‚˜íƒ€ë‚´ë©°, Normì´ 0ì— ê°€ê¹Œì›Œì§€ë©´ ë‘ ë¶„í¬ê°€ ê·¼ì‚¬ì ìœ¼ë¡œ ì¼ì¹˜í•¨ì„ ì˜ë¯¸í•œë‹¤.\ní•™ìŠµëœ ë¶„í¬ì™€ train data set ì‚¬ì´ì˜ ì°¨ì´ë¥¼ í™•ì¸í•œë‹¤. $\\begin{aligned} \\left| p_\\theta(\\mathbf{s}t) - p{\\text{train}}(\\mathbf{s}t) \\right| \u0026= |(1 - \\epsilon)^t p{\\text{train}}(\\mathbf{s}t) + (1 - (1 - \\epsilon)^t)p{\\text{mistake}}(\\mathbf{s}t) - p{\\text{train}}(\\mathbf{s}t)| \\ \u0026= |-(1 - (1 - \\epsilon)^t)p{\\text{train}}(\\mathbf{s}t) + (1 - (1 - \\epsilon)^t)p{\\text{mistake}}(\\mathbf{s}t)| \\ \u0026= |(1 - (1 - \\epsilon)^t)(p{\\text{mistake}}(\\mathbf{s}t) - p{\\text{train}}(\\mathbf{s}t))| \\ \u0026= (1 - (1 - \\epsilon)^t) \\left| p{\\text{mistake}}(\\mathbf{s}t) - p{\\text{train}}(\\mathbf{s}_t) \\right| \\end{aligned}$\n$(1 - \\epsilon)^t \\geq 1 - \\epsilon t \\text{ for } \\epsilon \\in [0, 1]$ ì„ì„ í™œìš©í•˜ì—¬ $1-(1-\\epsilon)^t \\approx \\epsilon t$ë¥¼ ê³„ì‚°í•œë‹¤.\n$\\begin{aligned} (1-\\epsilon)^t \u0026\\geq 1 - \\epsilon t \\ \u0026\\Rightarrow -1+(1-\\epsilon)^t \\geq - \\epsilon t \\ \u0026\\Rightarrow 1 - (1-\\epsilon)^t \\leq \\epsilon t \\ \u0026\\therefore 1-(1-\\epsilon)^t \\approx \\epsilon t \\end{aligned}$\nì¦‰, ì•„ë˜ì™€ ê°™ì´ TV Normì„ ì •ë¦¬í•  ìˆ˜ ìˆë‹¤.\n$\\begin{aligned} \\left| p_\\theta(\\mathbf{s}t) - p{\\text{train}}(\\mathbf{s}t) \\right| = (1 - (1 - \\epsilon)^t) \\left| p{\\text{mistake}}(\\mathbf{s}t) - p{\\text{train}}(\\mathbf{s}_t) \\right| \u0026\\leq 2(1 - (1-\\epsilon)^t) \\\u0026\\leq 2 \\epsilon t \\end{aligned}$\n2ë°°ì¸ ì§ê´€ì ì¸ ì´ìœ \n$p_\\theta(\\mathbf{s}), p_{train}(\\mathbf{s})$ì€ í™•ë¥  ë¶„í¬ì´ë¯€ë¡œ ìµœì†Ÿê°’ì€ 0, ìµœëŒ“ê°’ì€ 1ì´ë‹¤.\n$0 \\leq p_\\theta(\\mathbf{s}) \\leq 1, \\ 0 \\leq p_{train}(\\mathbf{s}) \\leq 1$\në‹¨ì¼ ìƒíƒœì¼ ê²½ìš° $\\left| p_\\theta(\\mathbf{s}t) - p{\\text{train}}(\\mathbf{s}_t) \\right|$ì˜ ìµœëŒ“ê°’ì€ 1ì´ë‹¤. (í•œìª½ì´ 1, ë‹¤ë¥¸ ìª½ì´ 0)\nê·¸ëŸ¬ë‚˜ ë¶„í¬ ê°„ì˜ ì „ì²´ ì°¨ì´ë¥¼ í™•ì¸í•˜ìë©´ ìµœëŒ“ê°’ì€ 2ì´ë‹¤.\n$\\sum_s p_\\theta(\\mathbf{s}) = 1, \\ \\sum_s p_{train}(\\mathbf{s}) = 1$\në¶„í¬ ê°„ì— ì „í˜€ ê²¹ì¹˜ëŠ” ë¶€ë¶„ì´ ì—†ëŠ” ê²½ìš°ë¥¼ ê°€ì •í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.\n$p_\\theta(s_0) = 1, \\ p_\\theta(s_1) = 0$\n$p_{train}(s_0) = 0, \\ p_{train}(s_1) = 1$\n$\\sum_s \\left| p_\\theta(\\mathbf{s}) - p_{\\text{train}}(\\mathbf{s}) \\right| = \\left| 1 - 0 \\right| + \\left| 0 - 1 \\right| = 2$\n2ë°° ë’¤ì— $(1 - (1-\\epsilon)^t)$ì¸ ì´ìœ \nì–‘ë³€ì˜ ê³µí†µ ê³„ìˆ˜ë¥¼ ë§ì¶”ê¸° ìœ„í•´\nì‹œê°„ ì¶•ì— ëŒ€í•˜ì—¬ costë¥¼ ëª¨ë‘ í•©ì‚°í–ˆì„ ë•Œ $\\sum_t \\epsilon + 2 \\epsilon t$ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.\n$\\begin{aligned} \\sum_t \\epsilon + 2 \\epsilon t \u0026= \\epsilon \\sum_t^T t + 2 \\epsilon t \\ \u0026= \\epsilon \\frac{T(T+2)}{2} + 2 \\epsilon t \\\u0026= \\epsilon(\\frac{T(T+2)}{2} + 2t) \\\u0026\\text{if Tê°€ ì•„ì£¼ í¬ë‹¤ë©´,} \\\u0026\\approx O(\\epsilon T^2) \\end{aligned}$\nê²°ë¡ \n$T$ê°€ ì»¤ì§ˆ ìˆ˜ë¡ $\\epsilon$ì´ ì‘ì•„ë„ ëˆ„ì  costê°€ ì—„ì²­ ì»¤ì§„ë‹¤. Behavior cloningì„ ì‚¬ìš©í–ˆì„ ë•Œ, trainì—ì„œ ë²—ì–´ë‚œ ìƒí™©ì´ ë°œìƒí•˜ë©´ ëŒ€ì‘í•˜ì§€ ëª»í•˜ëŠ” ê²½ìš°ê°€ ë°œìƒí•  ìˆ˜ ìˆìŒì„ ê°•ì¡°í•œë‹¤. Imitation Learningì€ train dataì— ì ë‹¹í•œ ì‹¤ìˆ˜ì™€ ì˜¤ë¥˜ê°€ ìˆì–´ì•¼ ë” ì˜ ë™ì‘í•œë‹¤.\nbehavior cloningì„ í™œìš©í•œ Imitation learningì„ ì˜ ë™ì‘í•˜ê²Œ í•˜ëŠ” ë°©ë²•\ncollect and augment ì˜ë„ì ìœ¼ë¡œ ì‹¤ìˆ˜(mistakes)ì™€ ìˆ˜ì • ì‚¬í•­(corrections)ì„ ì¶”ê°€í•œë‹¤. ì‹¤ìˆ˜ë¥¼ í•™ìŠµì„ ë°©í•´í•˜ì§€ë§Œ ìˆ˜ì • ì‚¬í•­ì€ ë„ì™€ì¤€ë‹¤. ê·¸ëŸ¬ë‚˜ ìˆ˜ì • ì‚¬í•­ì€ ì¢…ì¢… ì‹¤ìˆ˜ ë³´ë‹¤ í¬ê²Œ ë°©í•´í•  ìˆ˜ ìˆë‹¤. ë°ì´í„° ì¦ê°•(data augmentation) Case Study trail following as classification - GIUSTI, Alessandro, et al. A machine learning approach to visual perception of forest trails for mobile robots.Â IEEE Robotics and Automation Letters, 2015, 1.2: 661-667. imitation with a cheap robot - RAHMATIZADEH, Rouhollah, et al. Vision-based multi-task manipulation for inexpensive robots using end-to-end learning from demonstration. In:Â 2018 IEEE international conference on robotics and automation (ICRA). IEEE, 2018. p. 3758-3765. ê°•ë ¥í•œ ëª¨ë¸ ì‚¬ìš©í•˜ê¸° Non-Markovian behavior\nsequence modelì„ í™œìš©í•˜ì—¬ ì „ì²´ ê¸°ë¡ì„ í•™ìŠµí•œë‹¤. Multimodeal behavior\nexpressive continuous distributionì„ ì‚¬ìš©í•œë‹¤. mixture of Gaussians ì¥ì : êµ¬í˜„í•˜ê¸° ì‰¬ì›€ ë‹¨ì : weightë¥¼ ì§€ì •í•´ì•¼ í•¨. latent variable models í•´ë‹¹ ë¶„ì•¼ì—ì„œ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” variational sutoencoder ëª¨ë¸ ì‚¬ìš©í•˜ê¸° diffusion models ê³ ì°¨ì› ì•¡ì…˜ ìŠ¤í˜ì´ìŠ¤ë¥¼ ì´ì‚°í™”í•œë‹¤. ì´ì‚°í™”ì˜ ë¬¸ì œ: 1ì°¨ì›ì€ ì‰¬ìš°ë‚˜ ê³ ì°¨ì›ì€ ì–´ë µë‹¤. í•´ê²° ë°©ì•ˆ: í•œ ë²ˆì— í•œ ì°¨ì›ì”© ì´ì‚°í™”í•˜ê¸° Autoregressive discretization sequence model blockì„ í™œìš©í•˜ì—¬ ì´ì‚°í™”í•˜ê¸° ì¥ì : ë³µì¡í•œ ë¶„í¬ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤. ë‹¨ì : ì‹œí€€ìŠ¤ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë¬´ê²ë‹¤. Study Case imitation with diffustion models - CHI, Cheng, et al. Diffusion policy: Visuomotor policy learning via action diffusion.Â The International Journal of Robotics Research, 2023, 02783649241273668. imitation with latent variables - ZHAO, Tony Z., et al. Learning fine-grained bimanual manipulation with low-cost hardware.Â arXiv preprint arXiv:2304.13705, 2023. imitation with transformers - BROHAN, Anthony, et al. Rt-1: Robotics transformer for real-world control at scale.Â arXiv preprint arXiv:2212.06817, 2022. Multi-task learning\nì–´ë–»ê²Œ ë‘ ë²ˆì§¸ ì¥ì†Œì— ë„ì°©í•œ ê²ƒì„ ì•Œ ìˆ˜ ìˆì„ê¹Œ? ì•„ì§ì€ ì˜ ëª¨ë¥´ê² ì§€ë§Œ ìƒë‹¹íˆ ì‹ ê¸°í•œ ë¶€ë¶„ì´ë‹¤. Study Case YU, Tianhe, et al. Unsupervised visuomotor control through distributional planning networks.Â arXiv preprint arXiv:1902.05542, 2019. GHOSH, Dibya, et al. Learning to reach goals via iterated supervised learning.Â arXiv preprint arXiv:1912.06088, 2019. ANDRYCHOWICZ, Marcin, et al. Hindsight experience replay.Â Advances in neural information processing systems, 2017, 30. ","wordCount":"1137","inLanguage":"en","image":"https://son-dongwoo.github.io/posts/lecture-2/images/cover.webp","datePublished":"2025-02-05T01:10:35+09:00","dateModified":"2025-02-05T01:10:35+09:00","author":{"@type":"Person","name":"Son-Dongwoo"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://son-dongwoo.github.io/posts/lecture-2/"},"publisher":{"@type":"Organization","name":"Son-Dongwoo's Blog","logo":{"@type":"ImageObject","url":"https://son-dongwoo.github.io/%3Clink%20/%20abs%20url%3E"}}}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})'></script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://son-dongwoo.github.io/ accesskey=h title="Son-Dongwoo's Blog (Alt + H)"><img src=https://son-dongwoo.github.io/apple-touch-icon.png alt aria-label=logo height=35>Son-Dongwoo's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://son-dongwoo.github.io/posts/ title="ğŸ“ posts"><span>ğŸ“ posts</span></a></li><li><a href=https://son-dongwoo.github.io/projects/ title="ğŸš€ projects"><span>ğŸš€ projects</span></a></li><li><a href=https://son-dongwoo.github.io/tags/ title="ğŸ·ï¸ tags"><span>ğŸ·ï¸ tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://son-dongwoo.github.io/>Home</a>&nbsp;Â»&nbsp;<a href=https://son-dongwoo.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Lecture 2, Imitation Learning</h1><div class=post-description>UC Berkeleyì˜ CS 285 ê°•ì˜ì¸ Deep Reinforcement Learningì˜ Lecture 2 ì •ë¦¬</div><div class=post-meta><span title='2025-02-05 01:10:35 +0900 KST'>February 5, 2025</span>&nbsp;Â·&nbsp;Son-Dongwoo</div><ul class=post-tags><li><a href=https://son-dongwoo.github.io/tags/deep-reinforcement-learning/>ğŸ·ï¸Deep Reinforcement Learning</a></li></ul></header><div class=post-content><blockquote><p>ê°•ì˜ ìë£Œ: <a href=https://rail.eecs.berkeley.edu/deeprlcourse/>UC Berkeley - Deep Reinforcement Learning</a><br>ê°•ì˜ ì˜ìƒ: <a href="https://www.youtube.com/playlist?list=PL_iWQOsE6TfVYGEGiAOMaOzzv41Jfm_Ps">Youtube Link</a></p></blockquote><ul><li><p>Terminology & notation</p><ul><li><p>$\mathbf{s}_t$: state</p></li><li><p>$\mathbf{o}_t$: observation</p></li><li><p>$\mathbf{a}_t$: action</p></li><li><p>$\pi_\theta(\mathbf{a}_t | \mathbf{o}_t)$: policy</p></li><li><p>$\pi_\theta(\mathbf{a}_t | \mathbf{s}_t)$: policy (fully observed)</p></li><li><p>$s_t$ì™€ $o_t$ëŠ” ë³„ê°œì´ë‹¤. ë‹¨, Imitation Learningì—ì„œëŠ” ëŒ€ë¶€ë¶„ ê°™ë‹¤.</p></li><li><p>$o_t$ì—ì„œ $s_t$ ë¥¼ ì™„ì „íˆ ì¶”ë¡ í•˜ì§€ ëª»í•˜ëŠ” ê²½ìš°ë„ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì•„ë˜ ì´ë¯¸ì§€ì—ì„œ ì¹˜íƒ€ë¥¼ ì¶”ë¡ í•˜ê³ ì í•œë‹¤. ìë™ì°¨ì— ê°€ë ¤ì ¸ ìˆì–´ ì¹˜íƒ€ë¥¼ ì¶”ë¡ í•˜ì§€ ëª»í•  ìˆ˜ ìˆë‹¤.</p><p><img alt=image-1.jpg loading=lazy src=/posts/lecture-2/images/image-1.png></p></li><li><p>ë°˜ë©´, $s_t$ì—ì„œ $o_t$ë¡œëŠ” í•­ìƒ ê³„ì‚° ê°€ëŠ¥í•˜ë‹¤.</p></li></ul></li><li><p>Markov property</p><ul><li>ë¯¸ë˜ëŠ” í˜„ì¬ì— ë”°ë¼ ê²°ì •ëœë‹¤. ê³¼ê±°ì™€ ë¬´ê´€í•˜ë‹¤.</li></ul></li><li><p>ê°•í™” í•™ìŠµì—ì„œì˜ í‘œê¸°ë²•ê³¼ ë¡œë´‡&ì œì–´ì—ì„œì˜ í‘œê¸°ë²•</p><table><thead><tr><th></th><th>ê°•í™” í•™ìŠµ</th><th>ë¡œë´‡&ì œì–´</th></tr></thead><tbody><tr><td>state</td><td>$\mathbf{s}_t$</td><td>$\mathbf{x}_t$</td></tr><tr><td>action</td><td>$\mathbf{a}_t$</td><td>$\mathbf{u}_t$</td></tr><tr><td>í•™ì</td><td>Richard Bellman(ë™ì  í”„ë¡œê·¸ë˜ë°(Dynamic Programming) ê°œë°œì)</td><td>Lev Pontryagin(ìµœì  ì œì–´ ì´ë¡ (Optimal Control Theory)ì˜ ì„ êµ¬ì)</td></tr></tbody></table></li><li><p>Behavioral cloningì´ ì˜ ì•ˆ ë˜ëŠ” ì´ìœ </p><ul><li>ê·¼ë³¸ì ì¸ ì´ìœ ëŠ” <a href="https://www.notion.so/Lecture-1-Introduction-180c8c7ad23c80d883f6f8acaf190bff?pvs=21">i.i.d. property</a> ë•Œë¬¸ì´ë‹¤.</li><li>í•™ìŠµ ì‹œ $\mathbf{o}<em>t$ ì—ì„œì˜ ë¼ë²¨ë§ì€ $\mathbf{o}</em>{t+1}$ ì— ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•ŠëŠ”ë‹¤. ê·¸ëŸ¬ë‚˜ í˜„ì‹¤ì—ì„œëŠ” ê³¼ê±°ì˜ ì„ íƒì´ ë¯¸ë˜ì˜ Observationì— ì˜í–¥ì„ ë¯¸ì¹˜ê¸° ë•Œë¬¸ì— ê²°ê³¼ì ìœ¼ë¡œ ì œëŒ€ë¡œ ë™ì‘í•˜ì§€ ì•ŠëŠ”ë‹¤.</li></ul><p><img alt="Images: Bojarski et al. â€˜16, NVIDIA" loading=lazy src=/posts/lecture-2/images/image-2.png></p><p>Images: Bojarski et al. â€˜16, NVIDIA</p><p><img alt="stateë¥¼ 1ì°¨ì›ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ì—†ì§€ë§Œ ì§ê´€ì ì¸ ì„¤ëª…ì„ ìœ„í•´ ê°€ì •í•˜ì˜€ë‹¤." loading=lazy src=/posts/lecture-2/images/image-3.png></p><p>stateë¥¼ 1ì°¨ì›ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ì—†ì§€ë§Œ ì§ê´€ì ì¸ ì„¤ëª…ì„ ìœ„í•´ ê°€ì •í•˜ì˜€ë‹¤.</p></li><li><p>Behavioral cloningì´ ì˜ ì•ˆ ë˜ëŠ” ì´ìœ ì˜ ì´ë¡ ì  ë°°ê²½</p><ul><li>The distributional shift problem<ul><li>train datasetì˜ í™•ë¥  ë¶„í¬ì™€ ìµœì¢… ì •ì±… í™•ë¥  ë¶„í¬ì˜ ì°¨ì´ë¡œ ì¸í•´ ì œëŒ€ë¡œ ë™ì‘í•˜ì§€ ì•ŠëŠ”ë‹¤.</li></ul></li></ul><p><img alt=image.jpg loading=lazy src=/posts/lecture-2/images/image-4.png></p><ul><li><p>ìˆ˜í•™ì  ë¶„ì„ì„ ìœ„í•´ ëª‡ ê°€ì§€ ê°€ì •ì„ í•œë‹¤.</p><ol><li><p>í•™ìŠµëœ ì •ì±… í™•ë¥  ë¶„í¬ì˜ ì¢‹ê³  ë‚˜ì¨ì„ íŒë‹¨í•˜ê¸° ìœ„í•´ cost function(reward function)ì„ ì •ì˜í•œë‹¤.</p><p>$c(\mathbf{s}_t,\mathbf{a}_t) =
\begin{cases}
0 & \text{if } \mathbf{a}_t = \pi^<em>(\mathbf{s}) \
1 & \text{otherwise}
\end{cases}$
($\pi^</em>(\mathbf{s})$ì€ ìš´ì „ìì˜ í–‰ë™ì´ ìµœì ì´ë¼ê³  ê°€ì •)</p></li><li><p>í•™ìŠµì˜ ëª©ì ì„ cost functionì˜ ìµœëŒ€í™”ë¡œ ì„¤ì •í•œë‹¤.</p><p>$\text{Goal: minimize } \mathbb{E}<em>{\mathbf{s}<em>t \sim p</em>{\pi</em>\theta}(\mathbf{s}_t)} \left[ c(\mathbf{s}_t, \mathbf{a}_t) \right]$</p></li><li><p>ì‘ì€ ì‹¤ìˆ˜ í™•ë¥  $\epsilon$ ì •ì˜: train data setì— ë“±ì¥í•˜ëŠ” ìƒíƒœ $\mathbf{s}$ì—ì„œëŠ” ìš´ì „ìì˜ í–‰ë™ $\pi^*(s)$ì™€ ë‹¤ë¥¸ í–‰ë™ì„ í•  í™•ë¥ ì´ $\epsilon$ ì´í•˜ë¼ê³  ê°€ì •í•œë‹¤. $\epsilon$ì€ ì‘ì§€ë§Œ 0ì´ ì•„ë‹Œ í™•ë¥ ì´ê¸°ì— â€˜ì‹¤ìˆ˜ê°€ ë°œìƒí•  ìˆ˜ ìˆì„â€™ì„ ì˜ë¯¸í•œë‹¤.</p><p>$\text{assume}: \pi_\theta(\mathbf{a} \neq \pi^*(\mathbf{s})|\mathbf{s}) \leq \epsilon
\\text{for all } \mathbf{s} \in \mathcal{D}_{train}$</p></li></ol></li><li><p>$T$ì— ëŒ€í•œ cost ê³„ì‚°</p><p>$E[\sum_t{c(\mathbf{s}_t, \mathbf{a}_t)}] \leq \epsilon T + (1 - \epsilon)(\epsilon (T - 1) + (1- \epsilon)(\ldots))$</p><p>â†’ $O(\epsilon T^2)$</p></li></ul></li></ul><aside style="border-radius:var(--radius);background:var(--code-bg);padding:5px;border-left:5px solid #f1c40f">ğŸ’¡ê³„ì‚° ë°©ì‹<ol><li>ì²« ìŠ¤í…œì—ì„œ ì‹¤ìˆ˜í•  í™•ë¥ ì€ $\epsilon$ ì´ë‹¤. ì´í›„ ë‚˜ë¨¸ì§€ ìŠ¤í…œ $T - 1$ ì „ì²´ê°€ ì‹¤ìˆ˜í•œë‹¤ê³  ìƒê°í•˜ë©´, ê¸°ì—¬ ë¹„ìš©(Contribution to cost)ëŠ” $\epsilon \times T$ ì´ë‹¤.</li><li>ì²« ìŠ¤í…ì— ì‹¤ìˆ˜ê°€ ì—†ì„ í™•ë¥ ì€ $(1 - \epsilon)$ì´ë‹¤. ë‘ ë²ˆì§¸ ìŠ¤í…ì—ì„œ ì‹¤ìˆ˜í•  í™•ë¥ ì€ $\epsilon$ ì´ë‹¤. ë‘ ë²ˆì§¸ ìŠ¤í…ë¶€í„° ì‹¤ìˆ˜í•œë‹¤ê³  ìƒê°í•˜ë©´, $T - 1$ ìŠ¤í… ë™ì•ˆ ì‹¤ìˆ˜í•˜ëŠ” ê²ƒì´ë‹¤. ì´ë•Œ ê¸°ì—¬ ë¹„ìš©ì€ $(1 - \epsilon)\epsilon \ \times \ (T - 1) \approx \epsilon T$</li><li>ì¦‰, ì „ì²´ $T$ì— ëŒ€í•œ costë¥¼ ê³„ì‚°í•˜ë©´ $O(\epsilon T^2)$ì„ì„ ì•Œ ìˆ˜ ìˆë‹¤.</li></ol></aside><ul><li><p>$O(\epsilon T^2)$ì— ëŒ€í•œ ì˜ë¯¸</p><ul><li>ì‘ì€ ì‹¤ìˆ˜ í™•ë¥  $\epsilon$ì´ ì¡´ì¬í•´ë„ $T$ê°€ ê¸¸ì–´ì§€ë©´, ìµœëŒ€ $O(\epsilon T^2)$ì´ë¼ëŠ” í° ëˆ„ì  costê°€ ë°œìƒí•  ìˆ˜ ìˆë‹¤.</li></ul></li><li><p>ê·¹ë‹¨ì ì¸ ì˜ˆì‹œ</p><ul><li>ê·¹ë‹¨ì ì¸ ì˜ˆì‹œë¡œ ì™¸ì¤„ íƒ€ê¸°ë¥¼ ë“¤ ìˆ˜ ìˆë‹¤.</li><li>ì™¸ì¤„ íƒ€ê¸°ì—ì„œ í•œ ë²ˆë§Œ ì‹¤ìˆ˜í•´ë„ ë°”ë‹¥ìœ¼ë¡œ ë–¨ì–´ì§„ë‹¤. ì¦‰, ì‹¤ìˆ˜ í•œ ë²ˆ ì´í›„ ìƒíƒœëŠ” ì „ë¶€ â€˜í•™ìŠµì´ ì•ˆ ë˜ì–´ ìˆëŠ” ìƒíƒœâ€™ê°€ ëœë‹¤. ë‚¨ì€ ì‹œê°„ ë™ì•ˆ ì‹¤ìˆ˜ê°€ ëˆ„ì ë  ìˆ˜ ìˆë‹¤.</li></ul></li></ul><div style=display:flex;justify-content:space-between><img src=images/image-5.png style=width:48%>
<img src=images/image-6.png style=width:48%></div><ul><li>í™•ë¥  ë¶„í¬ ê´€ì ì—ì„œì˜ ë¶„ì„<ul><li>$(1 - \epsilon)^t$: í•œ ë²ˆë„ ì‹¤ìˆ˜í•˜ì§€ ì•Šì„ í™•ë¥ </li><li>$p_{mistake}(\mathbf{s}_t)$: ì¡°ê¸ˆì´ë¼ë„ ì‹¤ìˆ˜ë¥¼ í•œ ë’¤ì— ê°€ê²Œ ë˜ëŠ” ë¶„í¬ (train data setì—ì„œ ë²—ì–´ë‚œ ìƒíƒœ)</li></ul></li></ul><p><img alt=image-7.jpg loading=lazy src=/posts/lecture-2/images/image-7.jpg></p><ul><li><p><a href=https://blog.naver.com/ycpiglet/223087981574>Total Variation Distance(TV Norm)</a>ë¥¼ í™œìš©í•œ ìˆ˜ì‹ ë³€í˜•</p><ul><li><p>TV Normì€ ë‘ ë¶„í¬ëŠ” ì–¼ë§ˆë‚˜ ì°¨ì´ë‚˜ëŠ”ê°€?ë¥¼ íŒë‹¨í•œë‹¤. ë‘ ë¶„í¬ì˜ ê±°ë¦¬ì˜ ìµœëŒ“ê°’ì„ ë‚˜íƒ€ë‚´ë©°, Normì´ 0ì— ê°€ê¹Œì›Œì§€ë©´ ë‘ ë¶„í¬ê°€ ê·¼ì‚¬ì ìœ¼ë¡œ ì¼ì¹˜í•¨ì„ ì˜ë¯¸í•œë‹¤.</p></li><li><p>í•™ìŠµëœ ë¶„í¬ì™€ train data set ì‚¬ì´ì˜ ì°¨ì´ë¥¼ í™•ì¸í•œë‹¤.
$\begin{aligned}
\left| p_\theta(\mathbf{s}<em>t) - p</em>{\text{train}}(\mathbf{s}<em>t) \right|
&= |(1 - \epsilon)^t p</em>{\text{train}}(\mathbf{s}<em>t) + (1 - (1 - \epsilon)^t)p</em>{\text{mistake}}(\mathbf{s}<em>t) - p</em>{\text{train}}(\mathbf{s}<em>t)| \
&= |-(1 - (1 - \epsilon)^t)p</em>{\text{train}}(\mathbf{s}<em>t) + (1 - (1 - \epsilon)^t)p</em>{\text{mistake}}(\mathbf{s}<em>t)| \
&= |(1 - (1 - \epsilon)^t)(p</em>{\text{mistake}}(\mathbf{s}<em>t) - p</em>{\text{train}}(\mathbf{s}<em>t))| \
&= (1 - (1 - \epsilon)^t) \left| p</em>{\text{mistake}}(\mathbf{s}<em>t) - p</em>{\text{train}}(\mathbf{s}_t) \right|
\end{aligned}$</p></li><li><p>$(1 - \epsilon)^t \geq 1 - \epsilon t \text{ for } \epsilon \in [0, 1]$ ì„ì„ í™œìš©í•˜ì—¬ $1-(1-\epsilon)^t \approx \epsilon t$ë¥¼ ê³„ì‚°í•œë‹¤.</p><p>$\begin{aligned}
(1-\epsilon)^t &\geq 1 - \epsilon t \
&\Rightarrow -1+(1-\epsilon)^t \geq - \epsilon t \
&\Rightarrow 1 - (1-\epsilon)^t \leq \epsilon t \
&\therefore 1-(1-\epsilon)^t \approx \epsilon t
\end{aligned}$</p></li><li><p>ì¦‰, ì•„ë˜ì™€ ê°™ì´ TV Normì„ ì •ë¦¬í•  ìˆ˜ ìˆë‹¤.</p><p>$\begin{aligned}
\left| p_\theta(\mathbf{s}<em>t) - p</em>{\text{train}}(\mathbf{s}<em>t) \right| = (1 - (1 - \epsilon)^t) \left| p</em>{\text{mistake}}(\mathbf{s}<em>t) - p</em>{\text{train}}(\mathbf{s}_t) \right| &\leq 2(1 - (1-\epsilon)^t) \&\leq 2 \epsilon t
\end{aligned}$</p></li><li><p>2ë°°ì¸ ì§ê´€ì ì¸ ì´ìœ </p><ul><li><p>$p_\theta(\mathbf{s}), p_{train}(\mathbf{s})$ì€ í™•ë¥  ë¶„í¬ì´ë¯€ë¡œ ìµœì†Ÿê°’ì€ 0, ìµœëŒ“ê°’ì€ 1ì´ë‹¤.</p><p>$0 \leq p_\theta(\mathbf{s}) \leq 1, \ 0 \leq p_{train}(\mathbf{s}) \leq 1$</p><p>ë‹¨ì¼ ìƒíƒœì¼ ê²½ìš° $\left| p_\theta(\mathbf{s}<em>t) - p</em>{\text{train}}(\mathbf{s}_t) \right|$ì˜ ìµœëŒ“ê°’ì€ 1ì´ë‹¤. (í•œìª½ì´ 1, ë‹¤ë¥¸ ìª½ì´ 0)</p></li><li><p>ê·¸ëŸ¬ë‚˜ ë¶„í¬ ê°„ì˜ ì „ì²´ ì°¨ì´ë¥¼ í™•ì¸í•˜ìë©´ ìµœëŒ“ê°’ì€ 2ì´ë‹¤.</p><p>$\sum_s p_\theta(\mathbf{s}) = 1, \ \sum_s p_{train}(\mathbf{s}) = 1$</p></li><li><p>ë¶„í¬ ê°„ì— ì „í˜€ ê²¹ì¹˜ëŠ” ë¶€ë¶„ì´ ì—†ëŠ” ê²½ìš°ë¥¼ ê°€ì •í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.</p><p>$p_\theta(s_0) = 1, \ p_\theta(s_1) = 0$</p><p>$p_{train}(s_0) = 0, \ p_{train}(s_1) = 1$</p><p>$\sum_s \left| p_\theta(\mathbf{s}) - p_{\text{train}}(\mathbf{s}) \right| = \left| 1 - 0 \right| + \left| 0 - 1 \right| = 2$</p></li></ul></li><li><p>2ë°° ë’¤ì— $(1 - (1-\epsilon)^t)$ì¸ ì´ìœ </p><p>ì–‘ë³€ì˜ ê³µí†µ ê³„ìˆ˜ë¥¼ ë§ì¶”ê¸° ìœ„í•´</p></li><li><p>ì‹œê°„ ì¶•ì— ëŒ€í•˜ì—¬ costë¥¼ ëª¨ë‘ í•©ì‚°í–ˆì„ ë•Œ $\sum_t \epsilon + 2 \epsilon t$ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.</p><p>$\begin{aligned}
\sum_t \epsilon + 2 \epsilon t &= \epsilon \sum_t^T t + 2 \epsilon t \
&= \epsilon \frac{T(T+2)}{2} + 2 \epsilon t
\&= \epsilon(\frac{T(T+2)}{2} + 2t)
\&\text{if Tê°€ ì•„ì£¼ í¬ë‹¤ë©´,}
\&\approx O(\epsilon T^2)
\end{aligned}$</p></li></ul></li><li><p>ê²°ë¡ </p><ul><li>$T$ê°€ ì»¤ì§ˆ ìˆ˜ë¡ $\epsilon$ì´ ì‘ì•„ë„ ëˆ„ì  costê°€ ì—„ì²­ ì»¤ì§„ë‹¤.</li><li>Behavior cloningì„ ì‚¬ìš©í–ˆì„ ë•Œ, trainì—ì„œ ë²—ì–´ë‚œ ìƒí™©ì´ ë°œìƒí•˜ë©´ ëŒ€ì‘í•˜ì§€ ëª»í•˜ëŠ” ê²½ìš°ê°€ ë°œìƒí•  ìˆ˜ ìˆìŒì„ ê°•ì¡°í•œë‹¤.</li></ul></li><li><p>Imitation Learningì€ train dataì— ì ë‹¹í•œ ì‹¤ìˆ˜ì™€ ì˜¤ë¥˜ê°€ ìˆì–´ì•¼ ë” ì˜ ë™ì‘í•œë‹¤.</p></li><li><p>behavior cloningì„ í™œìš©í•œ Imitation learningì„ ì˜ ë™ì‘í•˜ê²Œ í•˜ëŠ” ë°©ë²•</p><ol><li>collect and augment<ul><li>ì˜ë„ì ìœ¼ë¡œ ì‹¤ìˆ˜(mistakes)ì™€ ìˆ˜ì • ì‚¬í•­(corrections)ì„ ì¶”ê°€í•œë‹¤.<ul><li>ì‹¤ìˆ˜ë¥¼ í•™ìŠµì„ ë°©í•´í•˜ì§€ë§Œ ìˆ˜ì • ì‚¬í•­ì€ ë„ì™€ì¤€ë‹¤. ê·¸ëŸ¬ë‚˜ ìˆ˜ì • ì‚¬í•­ì€ ì¢…ì¢… ì‹¤ìˆ˜ ë³´ë‹¤ í¬ê²Œ ë°©í•´í•  ìˆ˜ ìˆë‹¤.</li></ul></li><li>ë°ì´í„° ì¦ê°•(data augmentation)</li><li>Case Study<ol><li>trail following as classification - GIUSTI, Alessandro, et al. A machine learning approach to visual perception of forest trails for mobile robots.Â <em>IEEE Robotics and Automation Letters</em>, 2015, 1.2: 661-667.</li><li>imitation with a cheap robot - RAHMATIZADEH, Rouhollah, et al. Vision-based multi-task manipulation for inexpensive robots using end-to-end learning from demonstration. In:Â <em>2018 IEEE international conference on robotics and automation (ICRA)</em>. IEEE, 2018. p. 3758-3765.</li></ol></li></ul></li><li>ê°•ë ¥í•œ ëª¨ë¸ ì‚¬ìš©í•˜ê¸°<ul><li><p>Non-Markovian behavior</p><ul><li>sequence modelì„ í™œìš©í•˜ì—¬ ì „ì²´ ê¸°ë¡ì„ í•™ìŠµí•œë‹¤.</li></ul><p><img alt=image-8.jpg loading=lazy src=/posts/lecture-2/images/image-8.jpg></p></li><li><p>Multimodeal behavior</p><ol><li>expressive continuous distributionì„ ì‚¬ìš©í•œë‹¤.<ol><li>mixture of Gaussians<ol><li>ì¥ì : êµ¬í˜„í•˜ê¸° ì‰¬ì›€</li><li>ë‹¨ì : weightë¥¼ ì§€ì •í•´ì•¼ í•¨.</li></ol></li><li>latent variable models<ol><li>í•´ë‹¹ ë¶„ì•¼ì—ì„œ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” variational sutoencoder ëª¨ë¸ ì‚¬ìš©í•˜ê¸°</li></ol></li><li>diffusion models</li></ol></li><li>ê³ ì°¨ì› ì•¡ì…˜ ìŠ¤í˜ì´ìŠ¤ë¥¼ ì´ì‚°í™”í•œë‹¤.<ol><li>ì´ì‚°í™”ì˜ ë¬¸ì œ: 1ì°¨ì›ì€ ì‰¬ìš°ë‚˜ ê³ ì°¨ì›ì€ ì–´ë µë‹¤.</li><li>í•´ê²° ë°©ì•ˆ: í•œ ë²ˆì— í•œ ì°¨ì›ì”© ì´ì‚°í™”í•˜ê¸°<ol><li>Autoregressive discretization<ol><li>sequence model blockì„ í™œìš©í•˜ì—¬ ì´ì‚°í™”í•˜ê¸°<ol><li>ì¥ì : ë³µì¡í•œ ë¶„í¬ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.</li><li>ë‹¨ì : ì‹œí€€ìŠ¤ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë¬´ê²ë‹¤.</li></ol></li></ol></li></ol></li></ol></li><li>Study Case<ol><li>imitation with diffustion models - CHI, Cheng, et al. Diffusion policy: Visuomotor policy learning via action diffusion.Â <em>The International Journal of Robotics Research</em>, 2023, 02783649241273668.</li><li>imitation with latent variables - ZHAO, Tony Z., et al. Learning fine-grained bimanual manipulation with low-cost hardware.Â <em>arXiv preprint arXiv:2304.13705</em>, 2023.</li><li>imitation with transformers - BROHAN, Anthony, et al. Rt-1: Robotics transformer for real-world control at scale.Â <em>arXiv preprint arXiv:2212.06817</em>, 2022.</li></ol></li></ol></li><li><p>Multi-task learning</p><p><img alt=image-9.jpg loading=lazy src=/posts/lecture-2/images/image-9.jpg></p><p><img alt=image-10.jpg loading=lazy src=/posts/lecture-2/images/image-10.jpg></p><ul><li>ì–´ë–»ê²Œ ë‘ ë²ˆì§¸ ì¥ì†Œì— ë„ì°©í•œ ê²ƒì„ ì•Œ ìˆ˜ ìˆì„ê¹Œ? ì•„ì§ì€ ì˜ ëª¨ë¥´ê² ì§€ë§Œ ìƒë‹¹íˆ ì‹ ê¸°í•œ ë¶€ë¶„ì´ë‹¤.</li><li>Study Case<ul><li>YU, Tianhe, et al. Unsupervised visuomotor control through distributional planning networks.Â <em>arXiv preprint arXiv:1902.05542</em>, 2019.</li><li>GHOSH, Dibya, et al. Learning to reach goals via iterated supervised learning.Â <em>arXiv preprint arXiv:1912.06088</em>, 2019.</li><li>ANDRYCHOWICZ, Marcin, et al. <strong>Hindsight experience replay</strong>.Â <em>Advances in neural information processing systems</em>, 2017, 30.</li></ul></li></ul></li></ul></li></ol></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://son-dongwoo.github.io/tags/deep-reinforcement-learning/>ğŸ·ï¸Deep Reinforcement Learning</a></li></ul><nav class=paginav><a class=next href=https://son-dongwoo.github.io/posts/lecture-1/><span class=title>Next Â»</span><br><span>Lecture 1, Introduction</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://son-dongwoo.github.io/>Son-Dongwoo's Blog</a></span> Â·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>