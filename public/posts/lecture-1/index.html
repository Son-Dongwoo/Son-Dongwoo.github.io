<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Lecture 1, Introduction | Son-Dongwoo's Blog</title>
<meta name=keywords content="Deep Reinforcement Learning"><meta name=description content="UC Berkeleyì˜ CS 285 ê°•ì˜ì¸ Deep Reinforcement Learningì˜ Lecture 1 ì •ë¦¬"><meta name=author content="Son-Dongwoo"><link rel=canonical href=https://son-dongwoo.github.io/posts/lecture-1/><meta name=google-site-verification content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://son-dongwoo.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://son-dongwoo.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://son-dongwoo.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://son-dongwoo.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://son-dongwoo.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://son-dongwoo.github.io/posts/lecture-1/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:url" content="https://son-dongwoo.github.io/posts/lecture-1/"><meta property="og:site_name" content="Son-Dongwoo's Blog"><meta property="og:title" content="Lecture 1, Introduction"><meta property="og:description" content="UC Berkeleyì˜ CS 285 ê°•ì˜ì¸ Deep Reinforcement Learningì˜ Lecture 1 ì •ë¦¬"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-02-05T01:10:32+09:00"><meta property="article:modified_time" content="2025-02-05T01:10:32+09:00"><meta property="article:tag" content="Deep Reinforcement Learning"><meta property="og:image" content="https://son-dongwoo.github.io/posts/lecture-1/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://son-dongwoo.github.io/posts/lecture-1/cover.png"><meta name=twitter:title content="Lecture 1, Introduction"><meta name=twitter:description content="UC Berkeleyì˜ CS 285 ê°•ì˜ì¸ Deep Reinforcement Learningì˜ Lecture 1 ì •ë¦¬"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://son-dongwoo.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Lecture 1, Introduction","item":"https://son-dongwoo.github.io/posts/lecture-1/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Lecture 1, Introduction","name":"Lecture 1, Introduction","description":"UC Berkeleyì˜ CS 285 ê°•ì˜ì¸ Deep Reinforcement Learningì˜ Lecture 1 ì •ë¦¬","keywords":["Deep Reinforcement Learning"],"articleBody":" ê°•ì˜ ìë£Œ: UC Berkeley - Deep Reinforcement Learning\nê°•ì˜ ì˜ìƒ: Youtube Link\nê°•í™” í•™ìŠµì€ ì‚¬ëŒì´ ìƒê°í•˜ì§€ ëª»í•˜ëŠ” ìƒˆë¡œìš´ í•´ê²°ì±…ì„ ì œì‹œí•  ìˆ˜ ìˆë‹¤. ê·¸ë ‡ê¸°ì— ê°•í™” í•™ìŠµì€ ê³µë¶€í•  ê°€ì¹˜ê°€ ì¶©ë¶„í•˜ë‹¤.\nê°•í™” í•™ìŠµì€ 2ê°€ì§€ ì˜ë¯¸ë¡œ ì •ì˜ë˜ë©°, ê° ë‚´ìš©ì€ ë¶„ë¦¬ëœ ì •ì˜ì´ë‹¤.\ní•™ìŠµ ê¸°ë°˜ ì˜ì‚¬ ê²°ì •ì„ ìœ„í•œ ìˆ˜í•™ì  í˜•ì‹ ì£¼ì˜\nâ†’ Mathematical formalism for learning-based decision making\nê²½í—˜ìœ¼ë¡œë¶€í„° ì˜ì‚¬ ê²°ì •ê³¼ ì œì–´ë¥¼ í•™ìŠµí•˜ëŠ” ì ‘ê·¼ ë°©ì‹\nâ†’ Approach for learning decision making and control from experience\nì§€ë„ í•™ìŠµê³¼ ê°•í™” í•™ìŠµì˜ ì°¨ì´\nì§€ë„ í•™ìŠµ (Usually assumes) ê°•í™” í•™ìŠµ i.i.d. data Data is not i.i.d. â†’ previous outputs influence future inputs! known ground truth outputs in training Ground truth answer is not known, only know if we succeeded or failed â†’ more generally, we know the reward ğŸ’¡\rWhat is i.i.d. data? i.i.d. (independent and identically distributed)\nIndependent (ë…ë¦½ì„±): ê° ë°ì´í„° ìƒ˜í”Œì€ ì„œë¡œ ë…ë¦½ì ì´ë‹¤. ì¦‰, í•œ ìƒ˜í”Œì´ ë‹¤ë¥¸ ìƒ˜í”Œì— ì˜í–¥ì„ ì£¼ì§€ ì•ŠëŠ”ë‹¤. Identically Distributed (ë™ì¼ ë¶„í¬): ëª¨ë“  ë°ì´í„° ìƒ˜í”Œì€ ë™ì¼í•œ í™•ë¥  ë¶„í¬ì—ì„œ ì¶”ì¶œëœë‹¤. ì´ëŠ” ë°ì´í„°ê°€ ì¼ì •í•œ ë¶„í¬ë¥¼ ë”°ë¥´ê³  ìˆë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. rewardëŠ” scalar ê°’!\nê°•í™” í•™ìŠµì€ Complex physical tasksë¥¼ ì˜í•œë‹¤.\në¦¬ì²˜ë“œ ì„œíŠ¼ì˜ **The Bitter Lesson** ì½ì–´ë³´ê¸°\nLearning: use data to extract patterns (Deep Learning) Search: use computation to extract inferences (Optimization) Imitation Learningì€ ë‹¨ìˆœíˆ ê·¼ìœ¡ì˜ ì›€ì§ì„ì„ ë”°ë¼í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ê·¸ í–‰ë™ì˜ ëª©ì ì„ ì´í•´í•˜ê³  ì—ì´ì „íŠ¸ê°€ ë…ì°½ì ìœ¼ë¡œ í–‰ë™í•˜ëŠ” ê²ƒì´ ì¸ìƒì ì´ë‹¤.\n","wordCount":"209","inLanguage":"en","image":"https://son-dongwoo.github.io/posts/lecture-1/cover.png","datePublished":"2025-02-05T01:10:32+09:00","dateModified":"2025-02-05T01:10:32+09:00","author":{"@type":"Person","name":"Son-Dongwoo"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://son-dongwoo.github.io/posts/lecture-1/"},"publisher":{"@type":"Organization","name":"Son-Dongwoo's Blog","logo":{"@type":"ImageObject","url":"https://son-dongwoo.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://son-dongwoo.github.io/ accesskey=h title="Son-Dongwoo's Blog (Alt + H)"><img src=https://son-dongwoo.github.io/apple-touch-icon.png alt aria-label=logo height=35>Son-Dongwoo's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://son-dongwoo.github.io/posts/ title="ğŸ“ posts"><span>ğŸ“ posts</span></a></li><li><a href=https://son-dongwoo.github.io/projects/ title="ğŸš€ projects"><span>ğŸš€ projects</span></a></li><li><a href=https://son-dongwoo.github.io/tags/ title="ğŸ·ï¸ tags"><span>ğŸ·ï¸ tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://son-dongwoo.github.io/>Home</a>&nbsp;Â»&nbsp;<a href=https://son-dongwoo.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Lecture 1, Introduction</h1><div class=post-description>UC Berkeleyì˜ CS 285 ê°•ì˜ì¸ Deep Reinforcement Learningì˜ Lecture 1 ì •ë¦¬</div><div class=post-meta><span title='2025-02-05 01:10:32 +0900 KST'>February 5, 2025</span>&nbsp;Â·&nbsp;1 min&nbsp;Â·&nbsp;Son-Dongwoo</div><ul class=post-tags><li><a href=https://son-dongwoo.github.io/tags/deep-reinforcement-learning/>ğŸ·ï¸Deep Reinforcement Learning</a></li></ul></header><figure class=entry-cover><img loading=eager srcset="https://son-dongwoo.github.io/posts/lecture-1/cover.png 150w" sizes="(min-width: 768px) 720px, 100vw" src=https://son-dongwoo.github.io/posts/lecture-1/cover.png alt="ì´ê²ƒì€ ëŒ€í‘œ ì´ë¯¸ì§€ì…ë‹ˆë‹¤." width=150 height=112></figure><div class=post-content><blockquote><p>ê°•ì˜ ìë£Œ: <a href=https://rail.eecs.berkeley.edu/deeprlcourse/>UC Berkeley - Deep Reinforcement Learning</a><br>ê°•ì˜ ì˜ìƒ: <a href="https://www.youtube.com/playlist?list=PL_iWQOsE6TfVYGEGiAOMaOzzv41Jfm_Ps">Youtube Link</a></p></blockquote><ul><li><p>ê°•í™” í•™ìŠµì€ <strong>ì‚¬ëŒì´ ìƒê°í•˜ì§€ ëª»í•˜ëŠ” ìƒˆë¡œìš´ í•´ê²°ì±…</strong>ì„ ì œì‹œí•  ìˆ˜ ìˆë‹¤. ê·¸ë ‡ê¸°ì— ê°•í™” í•™ìŠµì€ ê³µë¶€í•  ê°€ì¹˜ê°€ ì¶©ë¶„í•˜ë‹¤.</p></li><li><p>ê°•í™” í•™ìŠµì€ 2ê°€ì§€ ì˜ë¯¸ë¡œ ì •ì˜ë˜ë©°, ê° ë‚´ìš©ì€ ë¶„ë¦¬ëœ ì •ì˜ì´ë‹¤.</p><ol><li><p>í•™ìŠµ ê¸°ë°˜ ì˜ì‚¬ ê²°ì •ì„ ìœ„í•œ ìˆ˜í•™ì  í˜•ì‹ ì£¼ì˜</p><p>â†’ Mathematical formalism for learning-based decision making</p></li><li><p>ê²½í—˜ìœ¼ë¡œë¶€í„° ì˜ì‚¬ ê²°ì •ê³¼ ì œì–´ë¥¼ í•™ìŠµí•˜ëŠ” ì ‘ê·¼ ë°©ì‹</p><p>â†’ Approach for learning decision making and control from experience</p></li></ol></li><li><p>ì§€ë„ í•™ìŠµê³¼ ê°•í™” í•™ìŠµì˜ ì°¨ì´</p><table><thead><tr><th>ì§€ë„ í•™ìŠµ (Usually assumes)</th><th>ê°•í™” í•™ìŠµ</th></tr></thead><tbody><tr><td>i.i.d. data</td><td>Data is <strong>not</strong> i.i.d.</td></tr><tr><td>â†’ previous outputs influence future inputs!</td><td></td></tr><tr><td>known ground truth outputs in training</td><td>Ground truth answer is not known, only know if we succeeded or failed</td></tr><tr><td>â†’ more generally, we know the reward</td><td></td></tr></tbody></table></li></ul><aside>ğŸ’¡<p>What is <strong>i.i.d. data</strong>?
i.i.d. (independent and identically distributed)</p><ul><li><strong>Independent (ë…ë¦½ì„±):</strong> ê° ë°ì´í„° ìƒ˜í”Œì€ ì„œë¡œ ë…ë¦½ì ì´ë‹¤. ì¦‰, í•œ ìƒ˜í”Œì´ ë‹¤ë¥¸ ìƒ˜í”Œì— ì˜í–¥ì„ ì£¼ì§€ ì•ŠëŠ”ë‹¤.</li><li><strong>Identically Distributed (ë™ì¼ ë¶„í¬):</strong> ëª¨ë“  ë°ì´í„° ìƒ˜í”Œì€ ë™ì¼í•œ í™•ë¥  ë¶„í¬ì—ì„œ ì¶”ì¶œëœë‹¤. ì´ëŠ” ë°ì´í„°ê°€ ì¼ì •í•œ ë¶„í¬ë¥¼ ë”°ë¥´ê³  ìˆë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.</li></ul></aside><ul><li><p>rewardëŠ” scalar ê°’!</p></li><li><p>ê°•í™” í•™ìŠµì€ Complex physical tasksë¥¼ ì˜í•œë‹¤.</p></li><li><p>ë¦¬ì²˜ë“œ ì„œíŠ¼ì˜ <a href=http://incompleteideas.net/IncIdeas/BitterLesson.html>**The Bitter Lesson</a>** ì½ì–´ë³´ê¸°</p><ul><li>Learning: use data to extract patterns (Deep Learning)</li><li>Search: use computation to extract inferences (Optimization)</li></ul></li><li><p>Imitation Learningì€ ë‹¨ìˆœíˆ ê·¼ìœ¡ì˜ ì›€ì§ì„ì„ ë”°ë¼í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ê·¸ í–‰ë™ì˜ ëª©ì ì„ ì´í•´í•˜ê³  ì—ì´ì „íŠ¸ê°€ ë…ì°½ì ìœ¼ë¡œ í–‰ë™í•˜ëŠ” ê²ƒì´ ì¸ìƒì ì´ë‹¤.</p></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://son-dongwoo.github.io/tags/deep-reinforcement-learning/>ğŸ·ï¸Deep Reinforcement Learning</a></li></ul><nav class=paginav><a class=next href=https://son-dongwoo.github.io/posts/hugo.toml-setting/><span class=title>Next Â»</span><br><span>hugo.toml ì„¤ì •</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://son-dongwoo.github.io/>Son-Dongwoo's Blog</a></span> Â·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>