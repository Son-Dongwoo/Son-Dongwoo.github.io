+++
date = '2025-02-05T01:10:35+09:00'
draft = false
title = 'Lecture 2, Imitation Learning'

tags = ["Deep Reinforcement Learning"]
description = "UC Berkeleyì˜ CS 285 ê°•ì˜ì¸ Deep Reinforcement Learningì˜ Lecture 2 ì •ë¦¬"

UseHugoToc = true
showtoc = true
tocopen = false

[cover]
image = "images/cover.webp"  # ê°™ì€ í´ë” ë‚´ ì´ë¯¸ì§€ ì‚¬ìš©
relative = true  # when using page bundles set this to true, í˜ì´ì§€ ë²ˆë“¤ ë‚´ë¶€ ì´ë¯¸ì§€ ì‚¬ìš©
+++

> ê°•ì˜ ìë£Œ: [UC Berkeley - Deep Reinforcement Learning](https://rail.eecs.berkeley.edu/deeprlcourse/)<br>ê°•ì˜ ì˜ìƒ: [Youtube Link](https://www.youtube.com/playlist?list=PL_iWQOsE6TfVYGEGiAOMaOzzv41Jfm_Ps)
> 

- Terminology & notation
    - $\mathbf{s}_t$: state
    - $\mathbf{o}_t$: observation
    - $\mathbf{a}_t$: action
    - $\pi_\theta(\mathbf{a}_t \mid \mathbf{o}_t)$: policy
    - $\pi_\theta(\mathbf{a}_t \mid \mathbf{s}_t)$: policy (fully observed)
    
    - $s_t$ì™€ $o_t$ëŠ” ë³„ê°œì´ë‹¤. ë‹¨, Imitation Learningì—ì„œëŠ” ëŒ€ë¶€ë¶„ ê°™ë‹¤.
    - $o_t$ì—ì„œ $s_t$ë¥¼ ì™„ì „íˆ ì¶”ë¡ í•˜ì§€ ëª»í•˜ëŠ” ê²½ìš°ë„ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì•„ë˜ ì´ë¯¸ì§€ì—ì„œ ì¹˜íƒ€ë¥¼ ì¶”ë¡ í•˜ê³ ì í•œë‹¤. ìë™ì°¨ì— ê°€ë ¤ì ¸ ìˆì–´ ì¹˜íƒ€ë¥¼ ì¶”ë¡ í•˜ì§€ ëª»í•  ìˆ˜ ìˆë‹¤.
        
        ![image-1.jpg](images/image-1.png)
        
    - ë°˜ë©´, $s_t$ì—ì„œ $o_t$ë¡œëŠ” í•­ìƒ ê³„ì‚° ê°€ëŠ¥í•˜ë‹¤.

- Markov property
    - ë¯¸ë˜ëŠ” í˜„ì¬ì— ë”°ë¼ ê²°ì •ëœë‹¤. ê³¼ê±°ì™€ ë¬´ê´€í•˜ë‹¤.

- ê°•í™” í•™ìŠµì—ì„œì˜ í‘œê¸°ë²•ê³¼ ë¡œë´‡&ì œì–´ì—ì„œì˜ í‘œê¸°ë²•
    
    |  | ê°•í™” í•™ìŠµ | ë¡œë´‡&ì œì–´ |
    | --- | --- | --- |
    | state | $\mathbf{s}_t$ | $\mathbf{x}_t$ |
    | action | $\mathbf{a}_t$ | $\mathbf{u}_t$ |
    | í•™ì | Richard Bellman(ë™ì  í”„ë¡œê·¸ë˜ë°(Dynamic Programming) ê°œë°œì) | Lev Pontryagin(ìµœì  ì œì–´ ì´ë¡ (Optimal Control Theory)ì˜ ì„ êµ¬ì) |

- Behavioral cloningì´ ì˜ ì•ˆ ë˜ëŠ” ì´ìœ 
    - ê·¼ë³¸ì ì¸ ì´ìœ ëŠ” [i.i.d. property](https://son-dongwoo.github.io/posts/lecture-1/) ë•Œë¬¸ì´ë‹¤.
    - í•™ìŠµ ì‹œ {{< math >}}$\mathbf{o}_t${{< /math >}}ì—ì„œì˜ ë¼ë²¨ë§ì€ {{< math >}}$\mathbf{o}_{t+1}${{< /math >}}ì— ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•ŠëŠ”ë‹¤. ê·¸ëŸ¬ë‚˜ í˜„ì‹¤ì—ì„œëŠ” ê³¼ê±°ì˜ ì„ íƒì´ ë¯¸ë˜ì˜ Observationì— ì˜í–¥ì„ ë¯¸ì¹˜ê¸° ë•Œë¬¸ì— ê²°ê³¼ì ìœ¼ë¡œ ì œëŒ€ë¡œ ë™ì‘í•˜ì§€ ì•ŠëŠ”ë‹¤.
    
    ![Images: Bojarski et al. â€˜16, NVIDIA](images/image-2.png)
    
    Images: Bojarski et al. â€˜16, NVIDIA
    
    ![stateë¥¼ 1ì°¨ì›ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ì—†ì§€ë§Œ ì§ê´€ì ì¸ ì„¤ëª…ì„ ìœ„í•´ ê°€ì •í•˜ì˜€ë‹¤.](images/image-3.png)
    
    stateë¥¼ 1ì°¨ì›ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ì—†ì§€ë§Œ ì§ê´€ì ì¸ ì„¤ëª…ì„ ìœ„í•´ ê°€ì •í•˜ì˜€ë‹¤.

- Behavioral cloningì´ ì˜ ì•ˆ ë˜ëŠ” ì´ìœ ì˜ ì´ë¡ ì  ë°°ê²½
    - The distributional shift problem
        - train datasetì˜ í™•ë¥  ë¶„í¬ì™€ ìµœì¢… ì •ì±… í™•ë¥  ë¶„í¬ì˜ ì°¨ì´ë¡œ ì¸í•´ ì œëŒ€ë¡œ ë™ì‘í•˜ì§€ ì•ŠëŠ”ë‹¤.
    
    ![image.jpg](images/image-4.png)
    
    - ìˆ˜í•™ì  ë¶„ì„ì„ ìœ„í•´ ëª‡ ê°€ì§€ ê°€ì •ì„ í•œë‹¤.
        1. í•™ìŠµëœ ì •ì±… í™•ë¥  ë¶„í¬ì˜ ì¢‹ê³  ë‚˜ì¨ì„ íŒë‹¨í•˜ê¸° ìœ„í•´ cost function(reward function)ì„ ì •ì˜í•œë‹¤.
            
            {{< math >}}$$ 
            c(\mathbf{s}_t,\mathbf{a}_t) = 
            \begin{cases} 
            0 & \text{if } \mathbf{a}_t = \pi^*(\mathbf{s}), \\[1mm]
            1 & \text{otherwise}
            \end{cases}
            $${{< /math >}}
            
            <center>($\pi^*(\mathbf{s})$ì€ ìš´ì „ìì˜ í–‰ë™ì´ ìµœì ì´ë¼ê³  ê°€ì •)<br><br>
            
        2. í•™ìŠµì˜ ëª©ì ì„ cost functionì˜ ìµœì†Œí™”ë¡œ ì„¤ì •í•œë‹¤.
            
            {{< math >}}$$
            \text{Goal: minimize } \mathbb{E}_{\mathbf{s}_t \sim p_{\pi_\theta}(\mathbf{s}_t)} \bigl[ c(\mathbf{s}_t, \mathbf{a}_t) \bigr]
            $${{< /math >}}
            
        3. ì‘ì€ ì‹¤ìˆ˜ í™•ë¥  $\epsilon$ ì •ì˜: train data setì— ë“±ì¥í•˜ëŠ” ìƒíƒœ $\mathbf{s}$ì—ì„œëŠ” ìš´ì „ìì˜ í–‰ë™ $\pi^*(\mathbf{s})$ì™€ ë‹¤ë¥¸ í–‰ë™ì„ í•  í™•ë¥ ì´ $\epsilon$ ì´í•˜ë¼ê³  ê°€ì •í•œë‹¤. $\epsilon$ì€ ì‘ì§€ë§Œ 0ì´ ì•„ë‹Œ í™•ë¥ ì´ê¸°ì— â€˜ì‹¤ìˆ˜ê°€ ë°œìƒí•  ìˆ˜ ìˆìŒâ€™ì„ ì˜ë¯¸í•œë‹¤.
            
            $$
            \text{assume}: \quad \pi_\theta\bigl(\mathbf{a} \neq \pi^*(\mathbf{s}) \mid \mathbf{s}\bigr) \leq \epsilon \quad \text{for all } \mathbf{s} \in \mathcal{D}_{\text{train}}
            $$
        
    - $T$ì— ëŒ€í•œ cost ê³„ì‚°
        
        $$
        E\Bigl[\sum_t c(\mathbf{s}_t, \mathbf{a}_t)\Bigr] \leq \epsilon T + (1 - \epsilon)\Bigl(\epsilon (T - 1) + (1- \epsilon)(\ldots)\Bigr)
        $$

        $$
        â†’ O(\epsilon T^2)
        $$

<aside style="border-radius: var(--radius); background:var(--code-bg); padding:5px 10px; margin-bottom:10px; border-left:5px solid #f1c40f;">
ğŸ’¡ê³„ì‚° ë°©ì‹

1. ì²« ìŠ¤í…œì—ì„œ ì‹¤ìˆ˜í•  í™•ë¥ ì€ $\epsilon$ ì´ë‹¤. ì´í›„ ë‚˜ë¨¸ì§€ ìŠ¤í…œ $T - 1$ ì „ì²´ê°€ ì‹¤ìˆ˜í•œë‹¤ê³  ìƒê°í•˜ë©´, ê¸°ì—¬ ë¹„ìš©(Contribution to cost)ëŠ” $\epsilon \times T$ ì´ë‹¤.
2. ì²« ìŠ¤í…ì— ì‹¤ìˆ˜ê°€ ì—†ì„ í™•ë¥ ì€ $(1 - \epsilon)$ì´ë‹¤. ë‘ ë²ˆì§¸ ìŠ¤í…ì—ì„œ ì‹¤ìˆ˜í•  í™•ë¥ ì€ $\epsilon$ ì´ë‹¤. ë‘ ë²ˆì§¸ ìŠ¤í…ë¶€í„° ì‹¤ìˆ˜í•œë‹¤ê³  ìƒê°í•˜ë©´, $T - 1$ ìŠ¤í… ë™ì•ˆ ì‹¤ìˆ˜í•˜ëŠ” ê²ƒì´ë‹¤. ì´ë•Œ ê¸°ì—¬ ë¹„ìš©ì€ $(1 - \epsilon)\epsilon \ \times \ (T - 1) \approx \epsilon T$ 
3. ì¦‰, ì „ì²´ $T$ì— ëŒ€í•œ costë¥¼ ê³„ì‚°í•˜ë©´ $O(\epsilon T^2)$ì„ì„ ì•Œ ìˆ˜ ìˆë‹¤. 
</aside>

- $O(\epsilon T^2)$ì— ëŒ€í•œ ì˜ë¯¸
    - ì‘ì€ ì‹¤ìˆ˜ í™•ë¥  $\epsilon$ì´ ì¡´ì¬í•´ë„ $T$ê°€ ê¸¸ì–´ì§€ë©´, ìµœëŒ€ $O(\epsilon T^2)$ì´ë¼ëŠ” í° ëˆ„ì  costê°€ ë°œìƒí•  ìˆ˜ ìˆë‹¤.

- ê·¹ë‹¨ì ì¸ ì˜ˆì‹œ
    - ê·¹ë‹¨ì ì¸ ì˜ˆì‹œë¡œ ì™¸ì¤„ íƒ€ê¸°ë¥¼ ë“¤ ìˆ˜ ìˆë‹¤.
    - ì™¸ì¤„ íƒ€ê¸°ì—ì„œ í•œ ë²ˆë§Œ ì‹¤ìˆ˜í•´ë„ ë°”ë‹¥ìœ¼ë¡œ ë–¨ì–´ì§„ë‹¤. ì¦‰, ì‹¤ìˆ˜ í•œ ë²ˆ ì´í›„ ìƒíƒœëŠ” ì „ë¶€ â€˜í•™ìŠµì´ ì•ˆ ë˜ì–´ ìˆëŠ” ìƒíƒœâ€™ê°€ ëœë‹¤. ë‚¨ì€ ì‹œê°„ ë™ì•ˆ ì‹¤ìˆ˜ê°€ ëˆ„ì ë  ìˆ˜ ìˆë‹¤.

<div style="display: flex; justify-content: space-between;">
    <img src="images/image-5.png" style="width: 48%;">
    <img src="images/image-6.png" style="width: 48%;">
</div>

- í™•ë¥  ë¶„í¬ ê´€ì ì—ì„œì˜ ë¶„ì„
    - $(1 - \epsilon)^t$: í•œ ë²ˆë„ ì‹¤ìˆ˜í•˜ì§€ ì•Šì„ í™•ë¥ 
    - $p_{mistake}(\mathbf{s}_t)$: ì¡°ê¸ˆì´ë¼ë„ ì‹¤ìˆ˜ë¥¼ í•œ ë’¤ì— ê°€ê²Œ ë˜ëŠ” ë¶„í¬ (train data setì—ì„œ ë²—ì–´ë‚œ ìƒíƒœ)

![image-7.jpg](images/image-7.jpg)

- [Total Variation Distance(TV Norm)](https://blog.naver.com/ycpiglet/223087981574)ë¥¼ í™œìš©í•œ ìˆ˜ì‹ ë³€í˜•
    - TV Normì€ ë‘ ë¶„í¬ê°€ ì–¼ë§ˆë‚˜ ì°¨ì´ë‚˜ëŠ”ê°€ë¥¼ íŒë‹¨í•œë‹¤. Normì´ 0ì— ê°€ê¹Œì›Œì§€ë©´ ë‘ ë¶„í¬ê°€ ê·¼ì‚¬ì ìœ¼ë¡œ ì¼ì¹˜í•¨ì„ ì˜ë¯¸í•œë‹¤.
    - í•™ìŠµëœ ë¶„í¬ì™€ train data set ì‚¬ì´ì˜ ì°¨ì´ë¥¼ í™•ì¸í•œë‹¤.
    
    {{< math >}}$$ 
    \begin{aligned}
    \left| p_\theta(\mathbf{s}_t) - p_{\text{train}}(\mathbf{s}_t) \right| 
    &= \Bigl|(1 - \epsilon)^t \, p_{\text{train}}(\mathbf{s}_t) + \Bigl(1 - (1 - \epsilon)^t\Bigr) \, p_{\text{mistake}}(\mathbf{s}_t) - p_{\text{train}}(\mathbf{s}_t)\Bigr| \\
    &= \Bigl| -\Bigl(1 - (1 - \epsilon)^t\Bigr) \, p_{\text{train}}(\mathbf{s}_t) + \Bigl(1 - (1 - \epsilon)^t\Bigr) \, p_{\text{mistake}}(\mathbf{s}_t) \Bigr| \\
    &= \Bigl|(1 - (1 - \epsilon)^t) \Bigl(p_{\text{mistake}}(\mathbf{s}_t) - p_{\text{train}}(\mathbf{s}_t)\Bigr)\Bigr| \\
    &= (1 - (1 - \epsilon)^t) \left| p_{\text{mistake}}(\mathbf{s}_t) - p_{\text{train}}(\mathbf{s}_t) \right|
    \end{aligned}
    $${{< /math >}}
        
    - $(1 - \epsilon)^t \geq 1 - \epsilon t$ for $\epsilon \in [0, 1]$ë¥¼ í™œìš©í•˜ì—¬ 
      
      {{< math >}}$$ 
      \begin{aligned}
      (1-\epsilon)^t &\geq 1 - \epsilon t, \\
      \Rightarrow\; 1 - (1-\epsilon)^t &\leq \epsilon t, \\
      \therefore\quad 1-(1-\epsilon)^t &\approx \epsilon t.
      \end{aligned}
      $${{< /math >}}
        
    - ì¦‰, ì•„ë˜ì™€ ê°™ì´ TV Normì„ ì •ë¦¬í•  ìˆ˜ ìˆë‹¤.
      
      {{< math >}}$$ 
      \begin{aligned}
      \left| p_\theta(\mathbf{s}_t) - p_{\text{train}}(\mathbf{s}_t) \right| 
      &= \Bigl(1 - (1 - \epsilon)^t\Bigr) \left| p_{\text{mistake}}(\mathbf{s}_t) - p_{\text{train}}(\mathbf{s}_t) \right| \\
      &\leq 2\Bigl(1 - (1-\epsilon)^t\Bigr) \\
      &\leq 2 \epsilon t.
      \end{aligned}
      $${{< /math >}}
        
    - 2ë°°ì¸ ì§ê´€ì ì¸ ì´ìœ   
      (ìƒëµ)
        
    - ì‹œê°„ ì¶•ì— ëŒ€í•˜ì—¬ costë¥¼ ëª¨ë‘ í•©ì‚°í–ˆì„ ë•Œ,
      
      $$ 
      \begin{aligned}
      \sum_t \epsilon + 2 \epsilon t 
      &= \epsilon \sum_{t=1}^{T} t + 2 \epsilon t \\
      &= \epsilon \frac{T(T+2)}{2} + 2 \epsilon t \\
      &= \epsilon\Bigl(\frac{T(T+2)}{2} + 2t\Bigr) \\
      &\text{if $T$ê°€ ì•„ì£¼ í¬ë‹¤ë©´,} \\
      &\approx O(\epsilon T^2)
      \end{aligned}
      $$
        
- ê²°ë¡ 
    - $T$ê°€ ì»¤ì§ˆ ìˆ˜ë¡ $\epsilon$ì´ ì‘ì•„ë„ ëˆ„ì  costê°€ ì—„ì²­ ì»¤ì§„ë‹¤.
    - Behavior cloningì„ ì‚¬ìš©í–ˆì„ ë•Œ, trainì—ì„œ ë²—ì–´ë‚œ ìƒí™©ì´ ë°œìƒí•˜ë©´ ëŒ€ì‘í•˜ì§€ ëª»í•˜ëŠ” ê²½ìš°ê°€ ë°œìƒí•  ìˆ˜ ìˆìŒì„ ê°•ì¡°í•œë‹¤.

- Imitation Learningì€ train dataì— ì ë‹¹í•œ ì‹¤ìˆ˜ì™€ ì˜¤ë¥˜ê°€ ìˆì–´ì•¼ ë” ì˜ ë™ì‘í•œë‹¤.

- behavior cloningì„ í™œìš©í•œ Imitation learningì„ ì˜ ë™ì‘í•˜ê²Œ í•˜ëŠ” ë°©ë²•
    1. collect and augment
        - ì˜ë„ì ìœ¼ë¡œ ì‹¤ìˆ˜(mistakes)ì™€ ìˆ˜ì • ì‚¬í•­(corrections)ì„ ì¶”ê°€í•œë‹¤.
            - ì‹¤ìˆ˜ë¥¼ í•™ìŠµì„ ë°©í•´í•˜ì§€ë§Œ ìˆ˜ì • ì‚¬í•­ì€ ë„ì™€ì¤€ë‹¤. ê·¸ëŸ¬ë‚˜ ìˆ˜ì • ì‚¬í•­ì€ ì¢…ì¢… ì‹¤ìˆ˜ ë³´ë‹¤ í¬ê²Œ ë°©í•´í•  ìˆ˜ ìˆë‹¤.
        - ë°ì´í„° ì¦ê°•(data augmentation)
        - Case Study
            1. trail following as classification - GIUSTI, Alessandro, et al. A machine learning approach to visual perception of forest trails for mobile robots.Â *IEEE Robotics and Automation Letters*, 2015, 1.2: 661-667.
            2. imitation with a cheap robot - RAHMATIZADEH, Rouhollah, et al. Vision-based multi-task manipulation for inexpensive robots using end-to-end learning from demonstration. In:Â *2018 IEEE international conference on robotics and automation (ICRA)*. IEEE, 2018. p. 3758-3765.
    2. ê°•ë ¥í•œ ëª¨ë¸ ì‚¬ìš©í•˜ê¸°
        - Non-Markovian behavior
            - sequence modelì„ í™œìš©í•˜ì—¬ ì „ì²´ ê¸°ë¡ì„ í•™ìŠµí•œë‹¤.
            
            ![image-8.jpg](images/image-8.jpg)
            
        - Multimodeal behavior
            1. expressive continuous distributionì„ ì‚¬ìš©í•œë‹¤.
                1. mixture of Gaussians
                    1. ì¥ì : êµ¬í˜„í•˜ê¸° ì‰¬ì›€
                    2. ë‹¨ì : weightë¥¼ ì§€ì •í•´ì•¼ í•¨.
                2. latent variable models
                    1. í•´ë‹¹ ë¶„ì•¼ì—ì„œ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” variational sutoencoder ëª¨ë¸ ì‚¬ìš©í•˜ê¸°
                3. diffusion models
            2. ê³ ì°¨ì› ì•¡ì…˜ ìŠ¤í˜ì´ìŠ¤ë¥¼ ì´ì‚°í™”í•œë‹¤. 
                1. ì´ì‚°í™”ì˜ ë¬¸ì œ: 1ì°¨ì›ì€ ì‰¬ìš°ë‚˜ ê³ ì°¨ì›ì€ ì–´ë µë‹¤.
                2. í•´ê²° ë°©ì•ˆ: í•œ ë²ˆì— í•œ ì°¨ì›ì”© ì´ì‚°í™”í•˜ê¸°
                    1. Autoregressive discretization
                        1. sequence model blockì„ í™œìš©í•˜ì—¬ ì´ì‚°í™”í•˜ê¸°
                            1. ì¥ì : ë³µì¡í•œ ë¶„í¬ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.
                            2. ë‹¨ì : ì‹œí€€ìŠ¤ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë¬´ê²ë‹¤.
            3. Study Case
                1. imitation with diffustion models - CHI, Cheng, et al. Diffusion policy: Visuomotor policy learning via action diffusion.Â *The International Journal of Robotics Research*, 2023, 02783649241273668.
                2. imitation with latent variables - ZHAO, Tony Z., et al. Learning fine-grained bimanual manipulation with low-cost hardware.Â *arXiv preprint arXiv:2304.13705*, 2023.
                3. imitation with transformers - BROHAN, Anthony, et al. Rt-1: Robotics transformer for real-world control at scale.Â *arXiv preprint arXiv:2212.06817*, 2022.
        - Multi-task learning
            
            ![image-9.jpg](images/image-9.jpg)
            
            ![image-10.jpg](images/image-10.jpg)
            
            - ì–´ë–»ê²Œ ë‘ ë²ˆì§¸ ì¥ì†Œì— ë„ì°©í•œ ê²ƒì„ ì•Œ ìˆ˜ ìˆì„ê¹Œ? ì•„ì§ì€ ì˜ ëª¨ë¥´ê² ì§€ë§Œ ìƒë‹¹íˆ ì‹ ê¸°í•œ ë¶€ë¶„ì´ë‹¤.
            - Study Case
                - YU, Tianhe, et al. Unsupervised visuomotor control through distributional planning networks.Â *arXiv preprint arXiv:1902.05542*, 2019.
                - GHOSH, Dibya, et al. Learning to reach goals via iterated supervised learning.Â *arXiv preprint arXiv:1912.06088*, 2019.
                - ANDRYCHOWICZ, Marcin, et al. **Hindsight experience replay**.Â *Advances in neural information processing systems*, 2017, 30.